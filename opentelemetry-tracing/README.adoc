ifdef::env-github[]
:artifactId: opentelemetry-tracing
endif::[]

//***********************************************************************************
// Enable the following flag to build README.html files for JBoss EAP product builds.
// Comment it out for WildFly builds.
//***********************************************************************************
//:ProductRelease:

//***********************************************************************************
// Enable the following flag to build README.html files for EAP XP product builds.
// Comment it out for WildFly or JBoss EAP product builds.
//***********************************************************************************
//:EAPXPRelease:

// This is a universal name for all releases
:ProductShortName: JBoss EAP
// Product names and links are dependent on whether it is a product release (CD or JBoss)
// or the WildFly project.
// The "DocInfo*" attributes are used to build the book links to the product documentation

ifdef::ProductRelease[]
// JBoss EAP release
:productName: JBoss EAP
:productNameFull: Red Hat JBoss Enterprise Application Platform
:productVersion: 8.0
:DocInfoProductNumber: {productVersion}
:WildFlyQuickStartRepoTag: 8.0.x
:productImageVersion: 8.0.0
:helmChartName: jboss-eap/eap8
endif::[]

ifdef::EAPXPRelease[]
// JBoss EAP XP release
:productName: JBoss EAP XP
:productNameFull: Red Hat JBoss Enterprise Application Platform expansion pack
:productVersion: 5.0
:DocInfoProductNumber: 8.0
:WildFlyQuickStartRepoTag: XP_5.0.0.GA
:productImageVersion: 5.0
:helmChartName: jboss-eap/eap-xp5
endif::[]

ifdef::ProductRelease,EAPXPRelease[]
:githubRepoUrl: https://github.com/jboss-developer/jboss-eap-quickstarts/
:githubRepoCodeUrl: https://github.com/jboss-developer/jboss-eap-quickstarts.git
:jbossHomeName: EAP_HOME
:DocInfoProductName: Red Hat JBoss Enterprise Application Platform
:DocInfoProductNameURL: red_hat_jboss_enterprise_application_platform
:DocInfoPreviousProductName: jboss-enterprise-application-platform
:quickstartDownloadName: {productNameFull} {productVersion} Quickstarts
:quickstartDownloadUrl: https://access.redhat.com/jbossnetwork/restricted/listSoftware.html?product=appplatform&downloadType=distributions
:helmRepoName: jboss-eap
:helmRepoUrl: https://jbossas.github.io/eap-charts/
// END ifdef::ProductRelease,EAPXPRelease[]
endif::[]

ifndef::ProductRelease,EAPXPRelease[]
// WildFly project
:productName: WildFly
:productNameFull: WildFly Application Server
:ProductShortName: {productName}
:jbossHomeName: WILDFLY_HOME
:productVersion: 35
:productImageVersion: 35.0
:githubRepoUrl: https://github.com/wildfly/quickstart/
:githubRepoCodeUrl: https://github.com/wildfly/quickstart.git
:WildFlyQuickStartRepoTag: 35.0.0.Beta1
:DocInfoProductName: Red Hat JBoss Enterprise Application Platform
:DocInfoProductNameURL: red_hat_jboss_enterprise_application_platform
:DocInfoProductNumber: 8.0
:DocInfoPreviousProductName: jboss-enterprise-application-platform
:helmRepoName: wildfly
:helmRepoUrl: http://docs.wildfly.org/wildfly-charts/
:helmChartName: wildfly/wildfly
// END ifndef::ProductRelease,EAPCDRelease,EAPXPRelease[]
endif::[]

:source: {githubRepoUrl}

// Values for Openshift S2i sections attributes
:CDProductName:  {productNameFull} for OpenShift
:CDProductShortName: {ProductShortName} for OpenShift
:CDProductTitle: {CDProductName}
:CDProductNameSentence: Openshift release for {ProductShortName}
:CDProductAcronym: {CDProductShortName}
:CDProductVersion: {productVersion}
:EapForOpenshiftBookName: {productNameFull} for OpenShift
:EapForOpenshiftOnlineBookName: {EapForOpenshiftBookName} Online
:xpaasproduct: {productNameFull} for OpenShift
:xpaasproductOpenShiftOnline: {xpaasproduct} Online
:xpaasproduct-shortname: {CDProductShortName}
:xpaasproductOpenShiftOnline-shortname: {xpaasproduct-shortname} Online
:ContainerRegistryName: Red Hat Container Registry
:EapForOpenshiftBookName: Getting Started with {ProductShortName} for OpenShift Container Platform
:EapForOpenshiftOnlineBookName: Getting Started with {ProductShortName} for OpenShift Online
:OpenShiftOnlinePlatformName: Red Hat OpenShift Container Platform
:OpenShiftOnlineName: Red Hat OpenShift Online
:ImagePrefixVersion: eap80
:ImageandTemplateImportBaseURL: https://raw.githubusercontent.com/jboss-container-images/jboss-eap-openshift-templates
:ImageandTemplateImportURL: {ImageandTemplateImportBaseURL}/{ImagePrefixVersion}/
:BuildImageStream: jboss-{ImagePrefixVersion}-openjdk11-openshift
:RuntimeImageStream: jboss-{ImagePrefixVersion}-openjdk11-runtime-openshift

// OpenShift repository and reference for quickstarts
:EAPQuickStartRepo: https://github.com/jboss-developer/jboss-eap-quickstarts
:EAPQuickStartRepoRef: 8.0.x
:EAPQuickStartRepoTag: EAP_8.0.0.GA
// Links to the OpenShift documentation
:LinkOpenShiftGuide: https://access.redhat.com/documentation/en-us/{DocInfoProductNameURL}/{DocInfoProductNumber}/html-single/getting_started_with_jboss_eap_for_openshift_container_platform/
:LinkOpenShiftOnlineGuide: https://access.redhat.com/documentation/en-us/{DocInfoProductNameURL}/{DocInfoProductNumber}/html-single/getting_started_with_jboss_eap_for_openshift_online/

ifdef::EAPXPRelease[]
// Attributes for XP releases
:EapForOpenshiftBookName: {productNameFull} for OpenShift
:EapForOpenshiftOnlineBookName: {productNameFull} for OpenShift Online
:xpaasproduct: {productNameFull} for OpenShift
:xpaasproductOpenShiftOnline: {productNameFull} for OpenShift Online
:xpaasproduct-shortname: {ProductShortName} for OpenShift
:xpaasproductOpenShiftOnline-shortname: {ProductShortName} for OpenShift Online
:ContainerRegistryName: Red Hat Container Registry
:EapForOpenshiftBookName: {productNameFull} for OpenShift
:EapForOpenshiftOnlineBookName: {productNameFull} for OpenShift Online
:ImagePrefixVersion: eap-xp3
:ImageandTemplateImportURL: {ImageandTemplateImportBaseURL}/{ImagePrefixVersion}/
:BuildImageStream: jboss-{ImagePrefixVersion}-openjdk11-openshift
:RuntimeImageStream: jboss-{ImagePrefixVersion}-openjdk11-runtime-openshift
// OpenShift repository and reference for quickstarts
:EAPQuickStartRepoRef: xp-5.0.x
// Links to the OpenShift documentation
:LinkOpenShiftGuide: https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/{DocInfoProductNumber}/html/using_eclipse_microprofile_in_jboss_eap/using-the-openshift-image-for-jboss-eap-xp_default
:LinkOpenShiftOnlineGuide: https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/{DocInfoProductNumber}/html/using_eclipse_microprofile_in_jboss_eap/using-the-openshift-image-for-jboss-eap-xp_default
endif::[]

ifndef::ProductRelease,EAPCDRelease,EAPXPRelease[]
:ImageandTemplateImportURL: https://raw.githubusercontent.com/wildfly/wildfly-s2i/v{productVersion}.0/
endif::[]

//*************************
// Other values
//*************************
:buildRequirements: Java SE 17.0 or later, and Maven 3.6.0 or later
:jbdsEapServerName: Red Hat JBoss Enterprise Application Platform 8.0
:javaVersion: Jakarta EE 10
ifdef::EAPXPRelease[]
:javaVersion: Eclipse MicroProfile
endif::[]
:githubRepoBranch: master
:guidesBaseUrl: https://github.com/jboss-developer/jboss-developer-shared-resources/blob/master/guides/
:useEclipseUrl: {guidesBaseUrl}USE_JBDS.adoc#use_red_hat_jboss_developer_studio_or_eclipse_to_run_the_quickstarts
:useEclipseDeployJavaClientDocUrl: {guidesBaseUrl}USE_JBDS.adoc#deploy_and_undeploy_a_quickstart_containing_server_and_java_client_projects
:useEclipseDeployEARDocUrl: {guidesBaseUrl}USE_JBDS.adoc#deploy_and_undeploy_a_quickstart_ear_project
:useProductHomeDocUrl: {guidesBaseUrl}USE_OF_{jbossHomeName}.adoc#use_of_product_home_and_jboss_home_variables
:configureMavenDocUrl: {guidesBaseUrl}CONFIGURE_MAVEN_JBOSS_EAP.adoc#configure_maven_to_build_and_deploy_the_quickstarts
:addUserDocUrl: {guidesBaseUrl}CREATE_USERS.adoc#create_users_required_by_the_quickstarts
:addApplicationUserDocUrl: {guidesBaseUrl}CREATE_USERS.adoc#add_an_application_user
:addManagementUserDocUrl: {guidesBaseUrl}CREATE_USERS.adoc#add_an_management_user
:startServerDocUrl: {guidesBaseUrl}START_JBOSS_EAP.adoc#start_the_jboss_eap_server
:configurePostgresDocUrl: {guidesBaseUrl}CONFIGURE_POSTGRESQL_JBOSS_EAP.adoc#configure_the_postgresql_database_for_use_with_the_quickstarts
:configurePostgresDownloadDocUrl: {guidesBaseUrl}CONFIGURE_POSTGRESQL_JBOSS_EAP.adoc#download_and_install_postgresql
:configurePostgresCreateUserDocUrl: {guidesBaseUrl}CONFIGURE_POSTGRESQL_JBOSS_EAP.adoc#create_a_database_user
:configurePostgresAddModuleDocUrl: {guidesBaseUrl}CONFIGURE_POSTGRESQL_JBOSS_EAP.adoc#add_the_postgres_module_to_the_jboss_eap_server
:configurePostgresDriverDocUrl: {guidesBaseUrl}CONFIGURE_POSTGRESQL_JBOSS_EAP.adoc#configure_the_postgresql_driver_in_the_jboss_eap_server
:configureBytemanDownloadDocUrl: {guidesBaseUrl}CONFIGURE_BYTEMAN.adoc#download_and_configure_byteman
:configureBytemanDisableDocUrl: {guidesBaseUrl}CONFIGURE_BYTEMAN.adoc#disable_the_byteman_script
:configureBytemanClearDocUrl: {guidesBaseUrl}CONFIGURE_BYTEMAN.adoc#clear_the_transaction_object_store
:configureBytemanQuickstartDocUrl: {guidesBaseUrl}CONFIGURE_BYTEMAN.adoc#configure_byteman_for_use_with_the_quickstarts
:configureBytemanHaltDocUrl: {guidesBaseUrl}CONFIGURE_BYTEMAN.adoc#use_byteman_to_halt_the_application[
:configureBytemanQuickstartsDocUrl: {guidesBaseUrl}CONFIGURE_BYTEMAN.adoc#configure_byteman_for_use_with_the_quickstarts

:EESubsystemNamespace: urn:jboss:domain:ee:4.0
:IiopOpenJdkSubsystemNamespace: urn:jboss:domain:iiop-openjdk:2.0
:MailSubsystemNamespace: urn:jboss:domain:mail:3.0
:SingletonSubsystemNamespace: urn:jboss:domain:singleton:1.0
:TransactionsSubsystemNamespace: urn:jboss:domain:transactions:4.0

// LinkProductDocHome: https://access.redhat.com/documentation/en/red-hat-jboss-enterprise-application-platform/
:LinkProductDocHome: https://access.redhat.com/documentation/en/jboss-enterprise-application-platform-continuous-delivery
:LinkConfigGuide: https://access.redhat.com/documentation/en-us/{DocInfoProductNameURL}/{DocInfoProductNumber}/html-single/configuration_guide/
:LinkDevelopmentGuide: https://access.redhat.com/documentation/en-us/{DocInfoProductNameURL}/{DocInfoProductNumber}/html-single/development_guide/
:LinkGettingStartedGuide: https://access.redhat.com/documentation/en-us/{DocInfoProductNameURL}/{DocInfoProductNumber}/html-single/getting_started_guide/
:LinkOpenShiftWelcome: https://docs.openshift.com/online/welcome/index.html
:LinkOpenShiftSignup: https://docs.openshift.com/online/getting_started/choose_a_plan.html
:OpenShiftTemplateName: JBoss EAP CD (no https)

:ConfigBookName: Configuration Guide
:DevelopmentBookName: Development Guide
:GettingStartedBookName: Getting Started Guide

:JBDSProductName: Red Hat CodeReady Studio
:JBDSVersion: 12.15
:LinkJBDSInstall:  https://access.redhat.com/documentation/en-us/red_hat_codeready_studio/{JBDSVersion}/html-single/installation_guide/
:JBDSInstallBookName: Installation Guide
:LinkJBDSGettingStarted: https://access.redhat.com/documentation/en-us/red_hat_codeready_studio/{JBDSVersion}/html-single/getting_started_with_codeready_studio_tools/
:JBDSGettingStartedBookName: Getting Started with CodeReady Studio Tools

// Enable Rendering of Glow configuration in plugin examples
:portedToGlow: true

= opentelemetry-tracing: OpenTelemetry Tracing QuickStart
:author: Jason Lee
:level: Beginner
:technologies: OpenTelemetry Tracing
:openshift: true

[abstract]
The `opentelemetry-tracing` quickstart demonstrates the use of the OpenTelemetry tracing specification in {productName}.

:standalone-server-type: default
:archiveType: war
:archiveName: {artifactId}
:restoreScriptName: restore-configuration.cli
:helm-install-prerequisites-openshift: ../opentelemetry-tracing/helm-install-prerequisites-openshift.adoc
:helm-install-prerequisites-kubernetes: ../opentelemetry-tracing/helm-install-prerequisites-kubernetes.adoc


== What is it?

OpenTelemetry is a set of APIs, SDKs, tooling and integrations that are designed for the creation and management of
telemetry data such as traces, metrics, and logs. OpenTelemetry support in {productName} is limited to traces only.
{productName}'s support of OpenTelemetry provides out of the box tracing of Jakarta REST calls, as well as
container-managed Jakarta REST Client invocations. Additionally, applications can have injected a `Tracer` instance
in order to create and manage custom `Span`s as a given application may require. These traces are exported
to an OpenTelemetry Collector instance listening on the same host.

== Architecture

In this quickstart, we have a collection of CDI beans and REST endpoints that  expose functionalities of the OpenTelemetry support in {productName}.

// Use of {jbossHomeName}
:leveloffset: +1

ifdef::requires-multiple-servers[]
[[use_of_jboss_home_name]]
= Use of the {jbossHomeName}_1, {jbossHomeName}_2, and QUICKSTART_HOME Variables

This quickstart requires that you clone your `__{jbossHomeName}__` installation directory and run two servers. The installation path is described in detail here: link:{useProductHomeDocUrl}[Use of __{jbossHomeName}__ and __JBOSS_HOME__ Variables].

In the following instructions, replace `__{jbossHomeName}_1__` with the path to your first {productName} server and replace `__{jbossHomeName}_2__` with the path to your second cloned {productName} server.

When you see the replaceable variable __QUICKSTART_HOME__, replace it with the path to the root directory of all of the quickstarts.
endif::[]

ifdef::optional-domain-or-multiple-servers[]
[[use_of_jboss_home_name]]
= Use of the {jbossHomeName}_1, {jbossHomeName}_2, and QUICKSTART_HOME Variables

When deploying this quickstart to a managed domain, replace `__{jbossHomeName}__` with the actual path to your {productName} installation. The installation path is described in detail here: link:{useProductHomeDocUrl}[Use of __{jbossHomeName}__ and __JBOSS_HOME__ Variables].

When deploying this quickstart to multiple standalone servers, this quickstart requires that you clone your `__{jbossHomeName}__` installation directory and run two servers. In the following instructions, replace `__{jbossHomeName}_1__` with the path to your first {productName} server and replace `__{jbossHomeName}_2__` with the path to your second cloned {productName} server.

When you see the replaceable variable __QUICKSTART_HOME__, replace it with the path to the root directory of all of the quickstarts.
endif::[]

ifndef::requires-multiple-servers,optional-domain-or-multiple-servers[]
[[use_of_jboss_home_name]]
= Use of the {jbossHomeName} and QUICKSTART_HOME Variables

In the following instructions, replace `__{jbossHomeName}__` with the actual path to your {productName} installation. The installation path is described in detail here: link:{useProductHomeDocUrl}[Use of __{jbossHomeName}__ and __JBOSS_HOME__ Variables].

When you see the replaceable variable __QUICKSTART_HOME__, replace it with the path to the root directory of all of the quickstarts.
endif::[]

:leveloffset!:

== Prerequisites

To complete this guide, you will need:

* less than 15 minutes
* JDK 11+ installed with `JAVA_HOME` configured appropriately
* Apache Maven 3.5.3+
* Docker Compose, or alternatively Podman Compose

:leveloffset: +1

ifdef::requires-multiple-servers[]
[[use_of_jboss_home_name]]
= Use of the {jbossHomeName}_1, {jbossHomeName}_2, and QUICKSTART_HOME Variables

This quickstart requires that you clone your `__{jbossHomeName}__` installation directory and run two servers. The installation path is described in detail here: link:{useProductHomeDocUrl}[Use of __{jbossHomeName}__ and __JBOSS_HOME__ Variables].

In the following instructions, replace `__{jbossHomeName}_1__` with the path to your first {productName} server and replace `__{jbossHomeName}_2__` with the path to your second cloned {productName} server.

When you see the replaceable variable __QUICKSTART_HOME__, replace it with the path to the root directory of all of the quickstarts.
endif::[]

ifdef::optional-domain-or-multiple-servers[]
[[use_of_jboss_home_name]]
= Use of the {jbossHomeName}_1, {jbossHomeName}_2, and QUICKSTART_HOME Variables

When deploying this quickstart to a managed domain, replace `__{jbossHomeName}__` with the actual path to your {productName} installation. The installation path is described in detail here: link:{useProductHomeDocUrl}[Use of __{jbossHomeName}__ and __JBOSS_HOME__ Variables].

When deploying this quickstart to multiple standalone servers, this quickstart requires that you clone your `__{jbossHomeName}__` installation directory and run two servers. In the following instructions, replace `__{jbossHomeName}_1__` with the path to your first {productName} server and replace `__{jbossHomeName}_2__` with the path to your second cloned {productName} server.

When you see the replaceable variable __QUICKSTART_HOME__, replace it with the path to the root directory of all of the quickstarts.
endif::[]

ifndef::requires-multiple-servers,optional-domain-or-multiple-servers[]
[[use_of_jboss_home_name]]
= Use of the {jbossHomeName} and QUICKSTART_HOME Variables

In the following instructions, replace `__{jbossHomeName}__` with the actual path to your {productName} installation. The installation path is described in detail here: link:{useProductHomeDocUrl}[Use of __{jbossHomeName}__ and __JBOSS_HOME__ Variables].

When you see the replaceable variable __QUICKSTART_HOME__, replace it with the path to the root directory of all of the quickstarts.
endif::[]

:leveloffset!:

== Steps

// Start the {productName} Standalone Server
:leveloffset: +1

[[start_the_eap_standalone_server]]
= Start the {productName} Standalone Server
//******************************************************************************
// Include this template if your quickstart requires a normal start of a single
// standalone server.
//
// You must define the `standalone-server-type`. Supported values are:
//    default
//    full
//    full-ha
//    ha
//    custom
//
// * For mobile applications, you can define the `mobileApp` variable in the
//   `README.adoc` file to add `-b 0.0.0.0` to the command line. This allows
//    external clients, such as phones, tablets, and desktops, to connect
//    to the application through through your local network
//    ::mobileApp: {artifactId}-service
//
//******************************************************************************

//******************************************************************************
// This template sets attributes for the different standalone server profiles.
//
// You must define the `standalone-server-type`. Supported values are:
//    default
//    full
//    full-ha
//    ha
//    microprofile
//    custom
//******************************************************************************

// Standalone server with the default profile.
ifeval::["{standalone-server-type}"=="default"]
:serverProfile: default profile
:configFileName: standalone/configuration/standalone.xml
:serverArguments:
endif::[]

// Standalone server with the full profile.
ifeval::["{standalone-server-type}"=="full"]
:serverProfile: full profile
:configFileName: standalone/configuration/standalone-full.xml
:serverArguments:  -c standalone-full.xml
endif::[]

// Standalone server with the full HA profile.
ifeval::["{standalone-server-type}"=="full-ha"]
:serverProfile: full HA profile
:configFileName: standalone/configuration/standalone-full-ha.xml
:serverArguments:  -c standalone-full-ha.xml
endif::[]

// Start the standalone server with the HA profile.
ifeval::["{standalone-server-type}"=="ha"]
:serverProfile: HA profile
:configFileName: standalone/configuration/standalone-ha.xml
:serverArguments:  -c standalone-ha.xml
endif::[]

// Start the standalone server with the Eclipse MicroProfile profile.
ifeval::["{standalone-server-type}"=="microprofile"]
:serverProfile: MicroProfile profile
:configFileName: standalone/configuration/standalone-microprofile.xml
:serverArguments:  -c standalone-microprofile.xml
endif::[]

// Standalone server with the custom profile.
// NOTE: This profile requires that you define the `serverArguments` variable
// within the quickstart README.adoc file. For example:
//  :serverArguments: --server-config=../../docs/examples/configs/standalone-xts.xml
ifeval::["{standalone-server-type}"=="custom"]
:serverProfile: custom profile
endif::[]

// If there is no match, use the default profile.
ifndef::serverProfile[]
:standalone-server-type:  default
:serverProfile: default profile
:configFileName: standalone/configuration/standalone.xml
:serverArguments:
endif::serverProfile[]

. Open a terminal and navigate to the root of the {productName} directory.
. Start the {productName} server with the {serverProfile} by typing the following command.
+
ifdef::uses-jaeger[]
[source,subs="+quotes,attributes+",options="nowrap"]
----
$ __JAEGER_REPORTER_LOG_SPANS=true JAEGER_SAMPLER_TYPE=const JAEGER_SAMPLER_PARAM=1__ __{jbossHomeName}__/bin/standalone.sh {serverArguments}
----
endif::[]
ifndef::uses-jaeger[]
[source,subs="+quotes,attributes+",options="nowrap"]
----
$ __{jbossHomeName}__/bin/standalone.sh {serverArguments}
----
endif::[]
+
NOTE: For Windows, use the `__{jbossHomeName}__\bin\standalone.bat` script.

ifdef::mobileApp[]
+
Adding `-b 0.0.0.0` to the above command allows external clients, such as phones, tablets, and desktops, to connect through your local network. For example:
+
[source,subs="+quotes,attributes+",options="nowrap"]
----
$ __{jbossHomeName}__/bin/standalone.sh {serverArguments} -b 0.0.0.0
----
endif::[]

:leveloffset!:

[[configure_the_server]]
=== Configure the Server

You enable OpenTelemetry by running JBoss CLI commands. For your convenience, this quickstart batches the commands into a `configure-opentelemtry.cli` script provided in the root directory of this quickstart.

. Before you begin, make sure you do the following:

* xref:back_up_standalone_server_configuration[Back up the {productName} standalone server configuration] as described above.
* xref:start_the_eap_standalone_server[Start the {productName} server with the standalone default profile] as described above.

. Review the `configure-opentelemtry.cli` file in the root of this quickstart directory. This script adds the configuration that enables OpenTelemetry for the quickstart components. Comments in the script describe the purpose of each block of commands.
. Open a new terminal, navigate to the root directory of this quickstart, and run the following command, replacing `__{jbossHomeName}__` with the path to your server:
+
[source,subs="+quotes,attributes+",options="nowrap"]
----
$ __{jbossHomeName}__/bin/jboss-cli.sh --connect --file=configure-opentelemetry.cli
----
+
NOTE: For Windows, use the `__{jbossHomeName}__\bin\jboss-cli.bat` script.
+

You should see the following result when you run the script:
+
[source,options="nowrap"]
----
The batch executed successfully
process-state: reload-required
----

. You'll need to reload the configuration after that:
+
[source,subs="+quotes,attributes+",options="nowrap"]
----
$ __{jbossHomeName}__/bin/jboss-cli.sh --connect --commands=reload
----

[[start_opentelemetry_collector]]
=== Starting the OpenTelemetry Collector

By default, {productName} will publish traces every 10 seconds, so you will soon start seeing errors about a refused connection.

This is because we told {productName} to publish to a server that is not there, so we need to fix that. To make that as simple as possible, you can use Docker Compose to start an instance of the OpenTelemetry Collector.

The Docker Compose configuration file is `docker-compose.yaml`:

[source,yaml]
----
version: "3"
volumes:
  shared-volume:
    # - logs:/var/log
services:
  otel-collector:
    image: otel/opentelemetry-collector:0.89.0
    command: [--config=/etc/otel-collector-config.yaml]
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml:Z
    ports:
      - 1888:1888 # pprof extension
      - 13133:13133 # health_check extension
      - 4317:4317 # OTLP gRPC receiver
      - 4318:4318 # OTLP http receiver
      - 55679:55679 # zpages extension
----

The Collector server configuration file is `otel-collector-config.yaml`:

[source,yaml]
----
extensions:
  health_check:
  pprof:
    endpoint: 0.0.0.0:1777
  zpages:
    endpoint: 0.0.0.0:55679

receivers:
  otlp:
    protocols:
      grpc:
          endpoint: "0.0.0.0:4317"
      http:
          endpoint: "0.0.0.0:4318"

processors:
  batch:

exporters:
  debug:
    verbosity: detailed
  prometheus:
      endpoint: "0.0.0.0:1234"
service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: []
      exporters: [debug]
    metrics:
      receivers: [ otlp ]
      processors: [ batch ]
      exporters: [ debug, prometheus ]
    logs:
      receivers: [ otlp ]
      processors: [ batch ]
      exporters: [ debug ]

  extensions: [health_check, pprof, zpages]

----

We can now bring up the collector instance:

[source,bash]
----
$ docker-compose up
----

The service should be available almost immediately, which you can verify by looking for the log entry `Everything is ready. Begin running and processing data.`.

[NOTE]
====
You may use Podman as alternative to Docker if you prefer, in such case the command should be `podman-compose up`.
====

[NOTE]
====
If your environment does not support Docker or Podman, please refer to https://opentelemetry.io/docs/collector/installation[Otel Collector documentation] for alternatives on how to install and run the OpenTelemetry Collector. Please ensure the same OpenTelemetry version as the one in the docker-compose.yaml above is used, otherwise such configuration may fail to work.
====

[NOTE]
=====
Part of the value of OpenTelemetry is its vendor-agnostic approach to exporting its various supported signals. As such,
this demo will only log the incoming traces, leaving the relaying of those signals to a downstream aggregation platform
as an exercise for the reader.
=====

Now we can start adding our custom spans from our application.

=== Creating traces

==== Implicit tracing of REST resources

The OpenTelemetry support in {productName} provides an implicit tracing of all Jakarta REST resources. That means that
for all applications, {productName} will automatically:

* extract the Span context from the incoming Jakarta REST request
* start a new Span on incoming Jakarta REST request and close it when the request is completed
* inject Span context to any outgoing Jakarta REST request
* start a Span for any outgoing Jakarta REST request and finish the Span when the request is completed

==== Explicit tracing

The OpenTelemetry API also supports explicit tracing should your application required it:

[source,java]
-----
package org.wildfly.quickstarts.opentelemetry;

import jakarta.enterprise.context.RequestScoped;
import jakarta.inject.Inject;

import io.opentelemetry.api.trace.Span;
import io.opentelemetry.api.trace.Tracer;

@RequestScoped
public class ExplicitlyTracedBean {

    @Inject
    private Tracer tracer;

    public String getHello() {
        Span prepareHelloSpan = tracer.spanBuilder("prepare-hello").startSpan();
        prepareHelloSpan.makeCurrent();

        String hello = "hello";

        Span processHelloSpan = tracer.spanBuilder("process-hello").startSpan();
        processHelloSpan.makeCurrent();

        hello = hello.toUpperCase();

        processHelloSpan.end();
        prepareHelloSpan.end();

        return hello;
    }
}
-----

:leveloffset: +1

[[build_and_deploy_the_quickstart]]
= Build and Deploy the Quickstart
//******************************************************************************
// Include this template if your quickstart does a normal deployment of a archive.
//
// * Define the `archiveType` variable in the quickstart README file.
//   Supported values:
//    :archiveType: ear
//    :archiveType: war
//    :archiveType: jar
//
// * To override the archive name, which defaults to the {artifactId),
//   define the `archiveName` variable, for example:
//    :archiveName: {artifactId}-service
//
// * To override the archive output directory,
//   define the `archiveDir` variable, for example:
//    :archiveDir: ear/target
//
// * To override the Maven command, define the `mavenCommand` variable,
//   for example:
//    :mavenCommand: clean install wildfly:deploy
//******************************************************************************

// The archive name defaults to the artifactId if not overridden
ifndef::archiveName[]
:archiveName: {artifactId}
endif::archiveName[]

// The archive type defaults to war if not overridden
ifndef::archiveType[]
:archiveType: war
endif::archiveType[]

// Define the archive file name as the concatenation of "archiveName" + "." + "archiveType+
:archiveFileName: {archiveName}.{archiveType}

// If they have not defined the target archive directory, make it the default for the archive type.
ifndef::archiveDir[]

ifeval::["{archiveType}"=="ear"]
:archiveDir: {artifactId}/ear/target
endif::[]

ifeval::["{archiveType}"=="war"]
:archiveDir: {artifactId}/target
endif::[]

ifeval::["{archiveType}"=="jar"]
:archiveDir: {artifactId}/target
endif::[]

endif::archiveDir[]

ifndef::mavenCommand[]
ifeval::["{archiveType}"=="ear"]
:mavenCommand: clean install
endif::[]

ifeval::["{archiveType}"=="war"]
:mavenCommand: clean package
endif::[]

ifeval::["{archiveType}"=="jar"]
:mavenCommand: clean install
endif::[]

endif::mavenCommand[]

. Make sure {productName} server is started.
. Open a terminal and navigate to the root directory of this quickstart.
ifdef::reactive-messaging[]
. Run this command to enable the MicroProfile Reactive Messaging functionality on the server
+
[source,subs="attributes+",options="nowrap"]
----
$ __{jbossHomeName}__/bin/jboss-cli.sh --connect --file=enable-reactive-messaging.cli
----
endif::reactive-messaging[]
. Type the following command to build the quickstart.
+
[source,subs="attributes+",options="nowrap"]
----
$ mvn {mavenCommand}
----

. Type the following command to deploy the quickstart.
+
[source,subs="attributes+",options="nowrap"]
----
$ mvn wildfly:deploy
----

This deploys the `{archiveDir}/{archiveFileName}` to the running instance of the server.

You should see a message in the server log indicating that the archive deployed successfully.

:leveloffset!:

== Access the quickstart application

You can either access the application via your browser at http://localhost:8080/opentelemetry-tracing/implicit-trace[], or http://localhost:8080/opentelemetry-tracing/explicit-trace[]. You can also access it from the command line:

[source,bash]
----
$ curl http://localhost:8080/opentelemetry-tracing/implicit-trace
$ curl http://localhost:8080/opentelemetry-tracing/explicit-trace
----

Either endpoint should return a simple document:

[source]
-----
hello
-----

=== View the traces

You can view the traces by looking at the Collector's log. You should see something like this:

[source]
-----
otel-collector_1  | 2023-12-13T21:05:28.002Z    info    TracesExporter  {"kind": "exporter", "data_type": "traces", "name": "logging", "resource spans": 1, "spans": 1}
otel-collector_1  | 2023-12-13T21:05:28.002Z    info    ResourceSpans #0
otel-collector_1  | Resource SchemaURL: https://opentelemetry.io/schemas/1.20.0
otel-collector_1  | Resource attributes:
otel-collector_1  |      -> service.name: Str(opentelemetry-tracing.war)
otel-collector_1  |      -> telemetry.sdk.language: Str(java)
otel-collector_1  |      -> telemetry.sdk.name: Str(opentelemetry)
otel-collector_1  |      -> telemetry.sdk.version: Str(1.29.0)
otel-collector_1  | ScopeSpans #0
otel-collector_1  | ScopeSpans SchemaURL:
otel-collector_1  | InstrumentationScope io.smallrye.opentelemetry 2.6.0
otel-collector_1  | Span #0
otel-collector_1  |     Trace ID       : c761e8fadec36d222adac36dcff1f4b1
otel-collector_1  |     Parent ID      :
otel-collector_1  |     ID             : 08f93dd25f75b5cd
otel-collector_1  |     Name           : GET /opentelemetry-tracing/implicit-trace
otel-collector_1  |     Kind           : Server
otel-collector_1  |     Start time     : 2023-12-13 21:05:20.560054393 +0000 UTC
otel-collector_1  |     End time       : 2023-12-13 21:05:20.621635685 +0000 UTC
otel-collector_1  |     Status code    : Unset
otel-collector_1  |     Status message :
otel-collector_1  | Attributes:
otel-collector_1  |      -> net.host.port: Int(8080)
otel-collector_1  |      -> http.scheme: Str(http)
otel-collector_1  |      -> http.method: Str(GET)
otel-collector_1  |      -> http.status_code: Int(200)
otel-collector_1  |      -> net.transport: Str(ip_tcp)
otel-collector_1  |      -> user_agent.original: Str(curl/8.2.1)
otel-collector_1  |      -> net.host.name: Str(localhost)
otel-collector_1  |      -> http.route: Str(/opentelemetry-tracing/implicit-trace)
otel-collector_1  |      -> http.target: Str(/opentelemetry-tracing/implicit-trace)
otel-collector_1  |      -> net.sock.host.addr: Str(127.0.0.1)
otel-collector_1  |     {"kind": "exporter", "data_type": "traces", "name": "logging"}
-----

// Server Distribution Testing
:leveloffset: +1

[[run_the_integration_tests_with_server_distribution]]
= Run the Integration Tests
ifndef::integrationTestsDirectory[:integrationTestsDirectory: src/test/]
ifndef::extraStandardDistTestParams[:extraStandardDistTestParams: ]

This quickstart includes integration tests, which are located under the `{integrationTestsDirectory}` directory. The integration tests verify that the quickstart runs correctly when deployed on the server.

Follow these steps to run the integration tests.

. Make sure {productName} server is started.
. Make sure the quickstart is deployed.
. Type the following command to run the `verify` goal with the `integration-testing` profile activated.
+
[source,subs="attributes+",options="nowrap"]
----
$ mvn verify -Pintegration-testing {extraStandardDistTestParams}
----

:leveloffset!:
// Undeploy the Quickstart
:leveloffset: +2

[[undeploy_the_quickstart]]
= Undeploy the Quickstart

//*******************************************************************************
// Include this template if your quickstart does a normal undeployment of an archive.
//*******************************************************************************
When you are finished testing the quickstart, follow these steps to undeploy the archive.

. Make sure {productName} server is started.
. Open a terminal and navigate to the root directory of this quickstart.
. Type this command to undeploy the archive:
+
[source,options="nowrap"]
----
$ mvn wildfly:undeploy
----

:leveloffset!:
//  Restore the {productName} Standalone Server Configuration
:leveloffset: +2

[[restore_the_standalone_server_configuration]]
= Restore the {productName} Standalone Server Configuration
//******************************************************************************
// Include this template if your quickstart does a normal restoration of a single
// standalone server configuration.
// * It provides a CLI script.
// * You can manually restore the backup copy.
//
// You must define the script file name using the `restoreScriptName` attribute.
// For example:
// :restoreScriptName: remove-configuration.cli
//******************************************************************************

You can restore the original server configuration using either of the following methods.

* You can xref:restore_standalone_server_configuration_using_cli[run the `{restoreScriptName}` script] provided in the root directory of this quickstart.
* You can xref:restore_standalone_server_configuration_manually[manually restore the configuration] using the backup copy of the configuration file.

[[restore_standalone_server_configuration_using_cli]]
== Restore the {productName} Standalone Server Configuration by Running the JBoss CLI Script

. xref:start_the_eap_standalone_server[Start the {productName} server] as described above.
. Open a new terminal, navigate to the root directory of this quickstart, and run the following command, replacing `__{jbossHomeName}__` with the path to your server:
+
[source,subs="+quotes,attributes+",options="nowrap"]
----
$ __{jbossHomeName}__/bin/jboss-cli.sh --connect --file={restoreScriptName}
----
+
NOTE: For Windows, use the `__{jbossHomeName}__\bin\jboss-cli.bat` script.

:leveloffset!:
// Restore the {productName} Standalone Server Configuration Manually
:leveloffset: +3

[[restore_standalone_server_configuration_manually]]
= Restore the {productName} Standalone Server Configuration Manually
//******************************************************************************
// Include this template if your quickstart does a normal manual restoration
// of a single standalone server.
//******************************************************************************

//******************************************************************************
// This template sets attributes for the different standalone server profiles.
//
// You must define the `standalone-server-type`. Supported values are:
//    default
//    full
//    full-ha
//    ha
//    microprofile
//    custom
//******************************************************************************

// Standalone server with the default profile.
ifeval::["{standalone-server-type}"=="default"]
:serverProfile: default profile
:configFileName: standalone/configuration/standalone.xml
:serverArguments:
endif::[]

// Standalone server with the full profile.
ifeval::["{standalone-server-type}"=="full"]
:serverProfile: full profile
:configFileName: standalone/configuration/standalone-full.xml
:serverArguments:  -c standalone-full.xml
endif::[]

// Standalone server with the full HA profile.
ifeval::["{standalone-server-type}"=="full-ha"]
:serverProfile: full HA profile
:configFileName: standalone/configuration/standalone-full-ha.xml
:serverArguments:  -c standalone-full-ha.xml
endif::[]

// Start the standalone server with the HA profile.
ifeval::["{standalone-server-type}"=="ha"]
:serverProfile: HA profile
:configFileName: standalone/configuration/standalone-ha.xml
:serverArguments:  -c standalone-ha.xml
endif::[]

// Start the standalone server with the Eclipse MicroProfile profile.
ifeval::["{standalone-server-type}"=="microprofile"]
:serverProfile: MicroProfile profile
:configFileName: standalone/configuration/standalone-microprofile.xml
:serverArguments:  -c standalone-microprofile.xml
endif::[]

// Standalone server with the custom profile.
// NOTE: This profile requires that you define the `serverArguments` variable
// within the quickstart README.adoc file. For example:
//  :serverArguments: --server-config=../../docs/examples/configs/standalone-xts.xml
ifeval::["{standalone-server-type}"=="custom"]
:serverProfile: custom profile
endif::[]

// If there is no match, use the default profile.
ifndef::serverProfile[]
:standalone-server-type:  default
:serverProfile: default profile
:configFileName: standalone/configuration/standalone.xml
:serverArguments:
endif::serverProfile[]

When you have completed testing the quickstart, you can restore the original server configuration by manually restoring the backup copy the configuration file.

. If it is running, stop the {productName} server.
. Replace the `__{jbossHomeName}__/{configFileName}` file with the backup copy of the file.

:leveloffset!:
// Build and run sections for other environments/builds
ifndef::ProductRelease,EAPXPRelease[]
:leveloffset: +1

[[build_and_run_the_quickstart_with_provisioned_server]]
= Building and running the quickstart application with provisioned {productName} server

ifndef::mavenServerProvisioningCommand[]
ifeval::["{archiveType}"=="ear"]
:mavenServerProvisioningCommand: clean install
endif::[]
ifeval::["{archiveType}"=="war"]
:mavenServerProvisioningCommand: clean package
endif::[]
ifeval::["{archiveType}"=="jar"]
:mavenServerProvisioningCommand: clean install
endif::[]
endif::mavenServerProvisioningCommand[]

ifndef::deploymentTargetDir[]
ifndef::deploymentDir[:deploymentTargetDir: target]
ifdef::deploymentDir[:deploymentTargetDir: {deploymentDir}/target]
endif::deploymentTargetDir[]

ifndef::extraStartParams[:extraStartParams: ]
ifndef::extraProvisioningTestParams[:extraProvisioningTestParams: ]

Instead of using a standard {productName} server distribution, you can alternatively provision a {productName} server to deploy and run the quickstart. The functionality is provided by the WildFly Maven Plugin, and you may find its configuration in the quickstart `pom.xml`:

[source,xml,subs="attributes+"]
----
        <profile>
            <id>provisioned-server</id>
            <activation>
                <activeByDefault>true</activeByDefault>
            </activation>
            <build>
                <plugins>
                    <plugin>
                        <groupId>org.wildfly.plugins</groupId>
                        <artifactId>wildfly-maven-plugin</artifactId>
                        <configuration>
                            <discover-provisioning-info>
                                <version>${version.server}</version>
                            </discover-provisioning-info>
                            <add-ons>...</add-ons>
                        </configuration>
                        <executions>
                            <execution>
                                <goals>
                                    <goal>package</goal>
                                </goals>
                            </execution>
                        </executions>
                    </plugin>
                    ...
                </plugins>
            </build>
        </profile>
----

When built, the provisioned {productName} server can be found in the `{deploymentTargetDir}/server` directory, and its usage is similar to a standard server distribution, with the simplification that there is never the need to specify the server configuration to be started.

Follow these steps to run the quickstart using the provisioned server.

.Procedure

. Make sure the server is provisioned.
+
[source,subs="attributes+",options="nowrap"]
----
$ mvn {mavenServerProvisioningCommand}
----

ifdef::addQuickstartUser[]
. Add the quickstart user:
+
[source,subs="+quotes,attributes+",options="nowrap"]
----
$ {deploymentTargetDir}/server/bin/add-user.sh -a -u 'quickstartUser' -p 'quickstartPwd1!' {app-group-command}
----
endif::[]

ifdef::addQuickstartAdmin[]
. Add the quickstart admin:
+
[source,subs="+quotes,attributes+",options="nowrap"]
----
$ {deploymentTargetDir}/server/bin/add-user.sh -a -u 'quickstartAdmin' -p 'adminPwd1!' {admin-group-command}
----
[NOTE]
====
For Windows, use the `__{jbossHomeName}__\bin\add-user.bat` script.
====
endif::[]

. Start the {productName} provisioned server, using the WildFly Maven Plugin `start` goal.
+
ifndef::deploymentDir[]
[source,subs="attributes+",options="nowrap"]
----
$ mvn wildfly:start {extraStartParams}
----
endif::[]
ifdef::deploymentDir[]
[source,subs="attributes+",options="nowrap"]
----
$ mvn -f {deploymentDir}/pom.xml wildfly:start {extraStartParams}
----
endif::[]

. Type the following command to run the integration tests.
+
[source,subs="attributes+",options="nowrap"]
----
$ mvn verify -Pintegration-testing {extraProvisioningTestParams}
----

. Shut down the {productName} provisioned server.
+
ifndef::deploymentDir[]
[source,subs="attributes+",options="nowrap"]
----
$ mvn wildfly:shutdown
----
endif::[]
ifdef::deploymentDir[]
[source,subs="attributes+",options="nowrap"]
----
$ mvn -f {deploymentDir}/pom.xml wildfly:shutdown
----
endif::[]

:leveloffset!:
endif::[]
// Bootable JAR
:leveloffset: +1

[[build_and_run_the_quickstart_with_bootable_jar]]
= Building and Running the quickstart application in a bootable JAR

//******************************************************************************
// This template sets attributes for the different standalone server profiles.
//
// You must define the `standalone-server-type`. Supported values are:
//    default
//    full
//    full-ha
//    ha
//    microprofile
//    custom
//******************************************************************************

// Standalone server with the default profile.
ifeval::["{standalone-server-type}"=="default"]
:serverProfile: default profile
:configFileName: standalone/configuration/standalone.xml
:serverArguments:
endif::[]

// Standalone server with the full profile.
ifeval::["{standalone-server-type}"=="full"]
:serverProfile: full profile
:configFileName: standalone/configuration/standalone-full.xml
:serverArguments:  -c standalone-full.xml
endif::[]

// Standalone server with the full HA profile.
ifeval::["{standalone-server-type}"=="full-ha"]
:serverProfile: full HA profile
:configFileName: standalone/configuration/standalone-full-ha.xml
:serverArguments:  -c standalone-full-ha.xml
endif::[]

// Start the standalone server with the HA profile.
ifeval::["{standalone-server-type}"=="ha"]
:serverProfile: HA profile
:configFileName: standalone/configuration/standalone-ha.xml
:serverArguments:  -c standalone-ha.xml
endif::[]

// Start the standalone server with the Eclipse MicroProfile profile.
ifeval::["{standalone-server-type}"=="microprofile"]
:serverProfile: MicroProfile profile
:configFileName: standalone/configuration/standalone-microprofile.xml
:serverArguments:  -c standalone-microprofile.xml
endif::[]

// Standalone server with the custom profile.
// NOTE: This profile requires that you define the `serverArguments` variable
// within the quickstart README.adoc file. For example:
//  :serverArguments: --server-config=../../docs/examples/configs/standalone-xts.xml
ifeval::["{standalone-server-type}"=="custom"]
:serverProfile: custom profile
endif::[]

// If there is no match, use the default profile.
ifndef::serverProfile[]
:standalone-server-type:  default
:serverProfile: default profile
:configFileName: standalone/configuration/standalone.xml
:serverArguments:
endif::serverProfile[]

ifndef::mavenServerProvisioningCommand[]
ifeval::["{archiveType}"=="ear"]
:mavenServerProvisioningCommand: clean install
endif::[]
ifeval::["{archiveType}"=="war"]
:mavenServerProvisioningCommand: clean package
endif::[]
ifeval::["{archiveType}"=="jar"]
:mavenServerProvisioningCommand: clean install
endif::[]
endif::mavenServerProvisioningCommand[]

You can use the WildFly Maven Plugin to build a {productName} bootable JAR to run this quickstart.

The quickstart `pom.xml` file contains a Maven profile named *bootable-jar*, which activates the bootable JAR packaging when provisioning {productName}, through the `<bootable-jar>true</bootable-jar>` configuration element:

[source,xml,subs="attributes+"]
----
      <profile>
          <id>bootable-jar</id>
          <activation>
              <activeByDefault>true</activeByDefault>
          </activation>
          <build>
              <plugins>
                  <plugin>
                      <groupId>org.wildfly.plugins</groupId>
                      <artifactId>wildfly-maven-plugin</artifactId>
                      <configuration>
                          <discover-provisioning-info>
                              <version>${version.server}</version>
                          </discover-provisioning-info>
                          <bootable-jar>true</bootable-jar>
                          <add-ons>...</add-ons>
                      </configuration>
                      <executions>
                          <execution>
                              <goals>
                                  <goal>package</goal>
                              </goals>
                          </execution>
                      </executions>
                  </plugin>
                  ...
              </plugins>
          </build>
      </profile>
----

The *bootable-jar* profile is activate by default, and when built the {productName} bootable jar file is named `{artifactId}-bootable.jar`, and may be found in the `target` directory.

.Procedure

. Ensure the bootable jar is built.
+
[source,subs="attributes+",options="nowrap"]
----
$ mvn clean {mavenServerProvisioningCommand}
----

. Start the {productName} bootable jar use the WildFly Maven Plugin `start-jar` goal.
+
[source,subs="attributes+",options="nowrap"]
----
$ mvn wildfly:start-jar
----
+
[NOTE]
====
You may also start the bootable jar without Maven, using the `java` command.
[source,subs="attributes+",options="nowrap"]
----
$ java -jar target/{artifactId}-bootable.jar
----
====

. Run the integration tests use the `verify` goal, with the `integration-testing` profile activated.
+
[source,subs="attributes+",options="nowrap"]
----
$ mvn verify -Pintegration-testing
----

. Shut down the {productName} bootable jar use the WildFly Maven Plugin `shutdown` goal.
+
[source,options="nowrap"]
----
$ mvn wildfly:shutdown
----


:leveloffset!:
// OpenShift
:leveloffset: +1

:cloud-platform: OpenShift
:openshift: true
ifndef::helm-app-name[]
:helm-app-name: {artifactId}
endif::helm-app-name[]

[[build_and_run_the_quickstart_on_openshift]]
= Building and running the quickstart application with OpenShift
// The openshift profile
:leveloffset: +1

[[build-the-quickstart-for-openshift]]
== Build the {productName} Source-to-Image (S2I) Quickstart to {cloud-platform} with Helm Charts

On OpenShift, the S2I build with Apache Maven uses an `openshift` Maven profile to provision a {productName} server, deploy and run the quickstart in OpenShift environment.

ifndef::ProductRelease,EAPXPRelease[]
The server provisioning functionality is provided by the WildFly Maven Plugin, and you may find its configuration in the quickstart `pom.xml`:
endif::[]
ifdef::ProductRelease,EAPXPRelease[]
The server provisioning functionality is provided by the EAP Maven Plugin, and you may find its configuration in the quickstart `pom.xml`:
endif::[]

ifndef::ProductRelease,EAPXPRelease[]

[source,xml,subs="attributes+"]
----
        <profile>
            <id>openshift</id>
            <build>
                <plugins>
                    <plugin>
                        <groupId>org.wildfly.plugins</groupId>
                        <artifactId>wildfly-maven-plugin</artifactId>
                        <configuration>
                            <discover-provisioning-info>
                                <version>${version.server}</version>
                                <context>cloud</context>
                            </discover-provisioning-info>
                            <add-ons>...</add-ons>
                        </configuration>
                        <executions>
                            <execution>
                                <goals>
                                    <goal>package</goal>
                                </goals>
                            </execution>
                        </executions>
                    </plugin>
                    ...
                </plugins>
            </build>
        </profile>
----
You may note that unlike the `provisioned-server` profile it uses the cloud context which enables a configuration tuned for {cloud-platform} environment.

The plugin uses https://github.com/wildfly/wildfly-glow[WildFly Glow] to discover the feature packs and layers required to run the application, and provisions a server containing those layers.

If you get an error or the server is missing some functionality which cannot be auto-discovered, you can download the https://github.com/wildfly/wildfly-glow/releases[WildFly Glow CLI] and run the following command to see more information about what add-ons are available:
[source,shell]
----
wildfly-glow show-add-ons
----
endif::ProductRelease,EAPXPRelease[]

ifdef::ProductRelease,EAPXPRelease[]
[source,xml,subs="attributes+"]
----
        <profile>
            <id>openshift</id>
            <build>
                <plugins>
                    <plugin>
                        <groupId>org.jboss.eap.plugins</groupId>
                        <artifactId>eap-maven-plugin</artifactId>
                        <configuration>
                            ...
                            <feature-packs>
                                <feature-pack>
                                    <location>org.jboss.eap:wildfly-ee-galleon-pack</location>
                                </feature-pack>
                                <feature-pack>
                                    <location>org.jboss.eap.cloud:eap-cloud-galleon-pack</location>
                                </feature-pack>
                            </feature-packs>
                            <layers>...</layers>
                        </configuration>
                        <executions>
                            <execution>
                                <goals>
                                    <goal>package</goal>
                                </goals>
                            </execution>
                        </executions>
                    </plugin>
                    ...
                </plugins>
            </build>
        </profile>
----
You may note that it uses the cloud feature pack which enables a configuration tuned for the {cloud-platform} environment.
endif::[]

:leveloffset: 1
// Getting Started with Helm
:leveloffset: +1

[[getting_started_with_helm]]
= Getting Started with {xpaasproduct-shortname} and Helm Charts

This section contains the basic instructions to build and deploy this quickstart to {xpaasproduct-shortname} or {xpaasproductOpenShiftOnline-shortname} using Helm Charts.

[[prerequisites_helm_openshift]]
== Prerequisites

ifndef::kubernetes[]
* You must be logged in OpenShift and have an `oc` client to connect to OpenShift
endif::[]
* https://helm.sh[Helm] must be installed to deploy the backend on {cloud-platform}.

Once you have installed Helm, you need to add the repository that provides Helm Charts for {productName}.

ifndef::ProductRelease,EAPXPRelease[]
[source,options="nowrap"]
----
$ helm repo add wildfly https://docs.wildfly.org/wildfly-charts/
"wildfly" has been added to your repositories
$ helm search repo wildfly
NAME                    CHART VERSION   APP VERSION     DESCRIPTION
wildfly/wildfly         ...             ...            Build and Deploy WildFly applications on OpenShift
wildfly/wildfly-common  ...             ...            A library chart for WildFly-based applications
----
endif::[]
ifdef::ProductRelease[]
[source,options="nowrap",subs="+attributes"]
----
$ helm repo add jboss-eap https://jbossas.github.io/eap-charts/
"jboss-eap" has been added to your repositories
$ helm search repo jboss-eap
NAME                    CHART VERSION   APP VERSION     DESCRIPTION
{helmChartName}         ...             ...             A Helm chart to build and deploy EAP {productVersion} applications
----
endif::[]
ifdef::EAPXPRelease[]
[source,options="nowrap",subs="+attributes"]
----
$ helm repo add jboss-eap https://jbossas.github.io/eap-charts/
"jboss-eap" has been added to your repositories
$ helm search repo jboss-eap
NAME                    CHART VERSION   APP VERSION     DESCRIPTION
{helmChartName}         ...             ...             A Helm chart to build and deploy EAP XP {productVersion} applications
----
endif::[]

:leveloffset: 1

ifdef::helm-install-prerequisites-openshift[]
// Additional steps needed before deploying in Helm
[[deploy_helm_prerequisites]]
:leveloffset: +1

:prereq-openshift: true
ifdef::prereq-openshift[]
:prereq-cloud-platform: OpenShift
:prereq-cloud-cli: oc
:prereq-suffix: openshift
endif::[]
ifdef::prereq-kubernetes[]
:prereq-cloud-platform: Kubernetes
:prereq-cloud-cli: kubectl
:prereq-suffix: kubernetes
endif::[]

=== Install OpenTelemetry Collector on {prereq-cloud-platform}

The functionality of this quickstart depends on a running instance of the https://opentelemetry.io/docs/collector/[OpenTelemetry Collector].

To deploy and configure the OpenTelemetry Collector, you will need to apply a set of configurations to your {prereq-cloud-platform} cluster, to configure the OpenTelemetry Collector as well as any external routes needed:

[source,options="nowrap",subs="+attributes"]
----
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: collector-config
data:
  collector.yml: |
    receivers:
      otlp:
        protocols:
          grpc:
              endpoint: "0.0.0.0:4317"
          http:
              endpoint: "0.0.0.0:4318"
    processors:
    exporters:
      debug:
        verbosity: detailed
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: []
          exporters: [debug]
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetrycollector
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: opentelemetrycollector
  template:
    metadata:
      labels:
        app.kubernetes.io/name: opentelemetrycollector
    spec:
      containers:
        - name: otelcol
          args:
            - --config=/conf/collector.yml
          image: otel/opentelemetry-collector:0.89.0
          volumeMounts:
            - mountPath: /conf
              name: collector-config
      volumes:
        - configMap:
            items:
              - key: collector.yml
                path: collector.yml
            name: collector-config
          name: collector-config
---
apiVersion: v1
kind: Service
metadata:
  name: opentelemetrycollector
spec:
  ports:
    - name: otlp-grpc
      port: 4317
      protocol: TCP
      targetPort: 4317
    - name: otlp-http
      port: 4318
      protocol: TCP
      targetPort: 4318
  selector:
    app.kubernetes.io/name: opentelemetrycollector
  type: ClusterIP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: otelcol-otlp-grpc
  labels:
    app.kubernetes.io/name: microprofile
spec:
  port:
    targetPort: otlp-grpc
  to:
    kind: Service
    name: opentelemetrycollector
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: otelcol-otlp-http
  labels:
    app.kubernetes.io/name: microprofile
spec:
  port:
    targetPort: otlp-http
  to:
    kind: Service
    name: opentelemetrycollector
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None
----

To make things simpler, you can find these commands in `charts/opentelemetry-collector-openshift.yaml`, and to apply them run the following command in your terminal:

[source,options="nowrap",subs="+attributes"]
----
$ {prereq-cloud-cli} apply -f charts/opentelemetry-collector-{prereq-suffix}.yaml
----

[NOTE]
====
When done with the quickstart, the `{prereq-cloud-cli} delete -f charts/opentelemetry-collector-{prereq-suffix}.yaml` command may be used to revert the applied changes.
====




:leveloffset: 1
endif::helm-install-prerequisites-openshift[]

//Prepare Helm for Quickstart Deployment
:leveloffset: +1

ifndef::helmSetWildFlyArgumentPrefix[]
// For use with nested Helm charts
:helmSetWildFlyArgumentPrefix:
endif::[]
ifeval::[{useHelmChartDir} == true]
:helm_chart_values: charts
endif::[]
ifndef::useHelmChartDir[]
:helm_chart_values: -f charts/helm.yaml {helmChartName}
endif::[]
ifdef::kubernetes[]
:helm-set-build-enabled: --set {helmSetWildFlyArgumentPrefix}build.enabled=false
:helm-set-deploy-route-enabled: --set {helmSetWildFlyArgumentPrefix}deploy.route.enabled=false
:helm-set-image-name: --set {helmSetWildFlyArgumentPrefix}image.name="localhost:5000/{artifactId}"
:helm-extra-arguments: {helm-set-build-enabled} {helm-set-deploy-route-enabled} {helm-set-image-name}
:cloud-cli: kubectl
endif::[]
ifndef::kubernetes[]
:helm-extra-arguments:
:cloud-cli: oc
endif::[]
[[deploy_helm]]
== Deploy the {ProductShortName} Source-to-Image (S2I) Quickstart to {cloud-platform} with Helm Charts

ifndef::kubernetes[]
Log in to your OpenShift instance using the `oc login` command.
endif::[]
The backend will be built and deployed on {cloud-platform} with a Helm Chart for {productName}.


ifndef::kubernetes[]
Navigate to the root directory of this quickstart and run the following command:
endif::[]
ifdef::kubernetes[]
Navigate to the root directory of this quickstart and run the following commands:

[source,options="nowrap",subs="+attributes"]
----
mvn -Popenshift package wildfly:image
----
This will use the `openshift` Maven profile we saw earlier to build the application, and create a Docker image containing the {productName} server with the application deployed. The name of the image will be `{artifactId}`.

Next we need to tag the image and make it available to Kubernetes. You can push it to a registry like `quay.io`. In this case we tag as `localhost:5000/{artifactId}:latest` and push it to the internal registry in our Kubernetes instance:

[source,options="nowrap",subs="+attributes"]
----
# Tag the image
docker tag {artifactId} localhost:5000/{artifactId}:latest
# Push the image to the registry
docker push localhost:5000/{artifactId}:latest
----

In the below call to `helm install` which deploys our application to Kubernetes, we are passing in some extra arguments to tweak the Helm build:

* `{helm-set-build-enabled}` - This turns off the s2i build for the Helm chart since Kubernetes, unlike OpenShift, does not have s2i. Instead, we are providing the image to use.
* `{helm-set-deploy-route-enabled}` - This disables route creation normally performed by the Helm chart. On Kubernetes we will use port-forwards instead to access our application, since routes are an OpenShift specific concept and thus not available on Kubernetes.
* `{helm-set-image-name}` - This tells the Helm chart to use the image we built, tagged and pushed to Kubernetes' internal registry above.

endif::[]
[source,options="nowrap",subs="+attributes"]
----
$ helm install {helm-app-name} {helm_chart_values} --wait --timeout=10m0s {helm-extra-arguments}
NAME: {helm-app-name}
...
STATUS: deployed
REVISION: 1
----

This command will return once the application has successfully deployed. In case of a timeout, you can check the status of the application with the following command in another terminal:

[source,options="nowrap",subs="+attributes"]
----
{cloud-cli} get deployment {helm-app-name}
----

The Helm Chart for this quickstart contains all the information to build an image from the source code using S2I on Java 17:


ifndef::requires-http-route[]
ifdef::useHelmChartDir[]
[source,yaml]
----
include::{docdir}/charts/Chart.yaml[]
----
endif::useHelmChartDir[]
ifndef::useHelmChartDir[]
[source,yaml]
----
build:
  uri: https://github.com/wildfly/quickstart.git
  ref: main
  contextDir: opentelemetry-tracing
deploy:
  replicas: 1
  env:
    - name: OTEL_COLLECTOR_HOST
      value: "opentelemetrycollector"
----
endif::useHelmChartDir[]
endif::requires-http-route[]

ifdef::requires-http-route[]
[source,options="nowrap",subs="+attributes"]
----
build:
  uri: {githubRepoCodeUrl}
  ref: {WildFlyQuickStartRepoTag}
  contextDir: {artifactId}
deploy:
  replicas: 1
  route:
    tls:
      enabled: false
----
endif::requires-http-route[]

This will create a new deployment on {cloud-platform} and deploy the application.

If you want to see all the configuration elements to customize your deployment you can use the following command:
[source,options="nowrap",subs="+attributes"]
----
$ helm show readme {helmChartName}
----

ifdef::openshift[]
Get the URL of the route to the deployment.

[source,options="nowrap",subs="+attributes"]
----
$ oc get route {helm-app-name} -o jsonpath="{.spec.host}"
----
Access the application in your web browser using the displayed URL.
endif::[]
ifdef::kubernetes[]
To be able to connect to our application running in Kubernetes from outside, we need to set up a port-forward to the `{helm-app-name}` service created for us by the Helm chart.

This service will run on port `8080`, and we set up the port forward to also run on port `8080`:

[source,options="nowrap",subs="+attributes"]
----
kubectl port-forward service/{helm-app-name} 8080:8080
----
The server can now be accessed via `http://localhost:8080` from outside Kubernetes. Note that the command to create the port-forward will not return, so it is easiest to run this in a separate terminal.

endif::[]

ifdef::openshift+post-helm-install-actions-openshift[]
include::{post-helm-install-actions-openshift}[leveloffset=+1]
endif::openshift+post-helm-install-actions-openshift[]
ifdef::kubernetes+post-helm-install-actions-kubernetes[]
include::{post-helm-install-actions-kubernetes}[leveloffset=+1]
endif::kubernetes+post-helm-install-actions-kubernetes[]

:leveloffset: 1

// Testing on Openshift
:leveloffset: +1

[[run_the_integration_tests_with_openshift]]
= Run the Integration Tests with OpenShift
The integration tests included with this quickstart, which verify that the quickstart runs correctly, may also be run with the quickstart running on OpenShift.
[NOTE]
====
The integration tests expect a deployed application, so make sure you have deployed the quickstart on OpenShift before you begin.
====

ifdef::extra-testing-actions-openshift[]
include::{extra-testing-actions-openshift}[leveloffset=+1]
endif::extra-testing-actions-openshift[]

ifndef::extra-test-arguments-openshift[:extra-test-arguments-openshift:]

Run the integration tests using the following command to run the `verify` goal with the `integration-testing` profile activated and the proper URL:
ifndef::requires-http-route[]
[source,options="nowrap",subs="+attributes"]
----
$ mvn verify -Pintegration-testing -Dserver.host=https://$(oc get route {helm-app-name} --template='{{ .spec.host }}') {extra-test-arguments-openshift}
----
endif::requires-http-route[]
ifdef::requires-http-route[]
[source,options="nowrap",subs="+attributes"]
----
$ mvn verify -Pintegration-testing -Dserver.host=http://$(oc get route {helm-app-name} --template='{{ .spec.host }}') {extra-test-arguments-openshift}
----
endif::requires-http-route[]

[NOTE]
====
The tests are using SSL to connect to the quickstart running on OpenShift. So you need the certificates to be trusted by the machine the tests are run from.
====

:leveloffset: 1

//Prepare Helm for Quickstart Deployment
:leveloffset: +1

[[undeploy_helm]]
== Undeploy the {ProductShortName} Source-to-Image (S2I) Quickstart from {cloud-platform} with Helm Charts

[source,options="nowrap",subs="+attributes"]
----
$ helm uninstall {helm-app-name}
----
ifdef::kubernetes[]
To stop the port forward you created earlier use:
[source,options="nowrap",subs="+attributes"]
----
$ kubectl port-forward service/{helm-app-name} 8080:8080
----
endif::[]

:leveloffset: 1

// Unset the attribute
:!openshift:

:leveloffset!:
ifndef::ProductRelease,EAPXPRelease[]
//Kubernetes
:leveloffset: +1

:cloud-platform: Kubernetes
:kubernetes: true
ifndef::helm-app-name[]
:helm-app-name: {artifactId}
endif::helm-app-name[]

[[build_and_run_the_quickstart_on_kubernetes]]
= Building and running the quickstart application with Kubernetes
// The openshift profile
:leveloffset: +1

[[build-the-quickstart-for-kubernetes]]
== Build the {productName} Quickstart to Kubernetes with Helm Charts

For Kubernetes, the build with Apache Maven uses an `openshift` Maven profile to provision a {productName} server, suitable for running on Kubernetes.

ifndef::ProductRelease,EAPXPRelease[]
The server provisioning functionality is provided by the WildFly Maven Plugin, and you may find its configuration in the quickstart `pom.xml`:
endif::[]
ifdef::ProductRelease,EAPXPRelease[]
The server provisioning functionality is provided by the EAP Maven Plugin, and you may find its configuration in the quickstart `pom.xml`:
endif::[]

ifndef::ProductRelease,EAPXPRelease[]

[source,xml,subs="attributes+"]
----
        <profile>
            <id>openshift</id>
            <build>
                <plugins>
                    <plugin>
                        <groupId>org.wildfly.plugins</groupId>
                        <artifactId>wildfly-maven-plugin</artifactId>
                        <configuration>
                            <discover-provisioning-info>
                                <version>${version.server}</version>
                                <context>cloud</context>
                            </discover-provisioning-info>
                            <add-ons>...</add-ons>
                        </configuration>
                        <executions>
                            <execution>
                                <goals>
                                    <goal>package</goal>
                                </goals>
                            </execution>
                        </executions>
                    </plugin>
                    ...
                </plugins>
            </build>
        </profile>
----
You may note that unlike the `provisioned-server` profile it uses the cloud context which enables a configuration tuned for {cloud-platform} environment.

The plugin uses https://github.com/wildfly/wildfly-glow[WildFly Glow] to discover the feature packs and layers required to run the application, and provisions a server containing those layers.

If you get an error or the server is missing some functionality which cannot be auto-discovered, you can download the https://github.com/wildfly/wildfly-glow/releases[WildFly Glow CLI] and run the following command to see more information about what add-ons are available:
[source,shell]
----
wildfly-glow show-add-ons
----
endif::ProductRelease,EAPXPRelease[]

ifdef::ProductRelease,EAPXPRelease[]
[source,xml,subs="attributes+"]
----
        <profile>
            <id>openshift</id>
            <build>
                <plugins>
                    <plugin>
                        <groupId>org.jboss.eap.plugins</groupId>
                        <artifactId>eap-maven-plugin</artifactId>
                        <configuration>
                            ...
                            <feature-packs>
                                <feature-pack>
                                    <location>org.jboss.eap:wildfly-ee-galleon-pack</location>
                                </feature-pack>
                                <feature-pack>
                                    <location>org.jboss.eap.cloud:eap-cloud-galleon-pack</location>
                                </feature-pack>
                            </feature-packs>
                            <layers>...</layers>
                        </configuration>
                        <executions>
                            <execution>
                                <goals>
                                    <goal>package</goal>
                                </goals>
                            </execution>
                        </executions>
                    </plugin>
                    ...
                </plugins>
            </build>
        </profile>
----
You may note that it uses the cloud feature pack which enables a configuration tuned for the {cloud-platform} environment.
endif::[]

:leveloffset: 1
// Getting Started with Helm
:leveloffset: +1

[[getting_started_with_helm]]
= Getting Started with Kubernetes and Helm Charts

This section contains the basic instructions to build and deploy this quickstart to Kubernetes using Helm Charts.

== Install Kubernetes
In this example we are using https://github.com/kubernetes/minikube[Minikube] as our Kubernetes provider. See the https://minikube.sigs.k8s.io/docs/start/[Minikube Getting Started guide] for how to install it. After installing it, we start it with 4GB of memory.

[source,options="nowrap",subs="+attributes"]
----
minikube start --memory='4gb'
----
The above command should work if you have Docker installed on your machine. If, you are using https://podman-desktop.io[Podman] instead of Docker, you will also need to pass in `--driver=podman`, as covered in the https://minikube.sigs.k8s.io/docs/handbook/config/[Minikube documentation].

Once Minikube has started, we need to enable its https://minikube.sigs.k8s.io/docs/handbook/registry/[registry] since that is where we will push the image needed to deploy the quickstart, and where we will tell the Helm charts to download it from.

[source,options="nowrap",subs="+attributes"]
----
minikube addons enable registry
----

In order to be able to push images to the registry we need to make it accessible from outside Kubernetes. How we do this depends on your operating system. All the below examples will expose it at `localhost:5000`

[source,options="nowrap",subs="+attributes"]
----
# On Mac:
docker run --rm -it --network=host alpine ash -c "apk add socat && socat TCP-LISTEN:5000,reuseaddr,fork TCP:$(minikube ip):5000"

# On Linux:
kubectl port-forward --namespace kube-system service/registry 5000:80 &

# On Windows:
kubectl port-forward --namespace kube-system service/registry 5000:80
docker run --rm -it --network=host alpine ash -c "apk add socat && socat TCP-LISTEN:5000,reuseaddr,fork TCP:host.docker.internal:5000"
----

[[prerequisites_helm_openshift]]
== Prerequisites

ifndef::kubernetes[]
* You must be logged in OpenShift and have an `oc` client to connect to OpenShift
endif::[]
* https://helm.sh[Helm] must be installed to deploy the backend on {cloud-platform}.

Once you have installed Helm, you need to add the repository that provides Helm Charts for {productName}.

ifndef::ProductRelease,EAPXPRelease[]
[source,options="nowrap"]
----
$ helm repo add wildfly https://docs.wildfly.org/wildfly-charts/
"wildfly" has been added to your repositories
$ helm search repo wildfly
NAME                    CHART VERSION   APP VERSION     DESCRIPTION
wildfly/wildfly         ...             ...            Build and Deploy WildFly applications on OpenShift
wildfly/wildfly-common  ...             ...            A library chart for WildFly-based applications
----
endif::[]
ifdef::ProductRelease[]
[source,options="nowrap",subs="+attributes"]
----
$ helm repo add jboss-eap https://jbossas.github.io/eap-charts/
"jboss-eap" has been added to your repositories
$ helm search repo jboss-eap
NAME                    CHART VERSION   APP VERSION     DESCRIPTION
{helmChartName}         ...             ...             A Helm chart to build and deploy EAP {productVersion} applications
----
endif::[]
ifdef::EAPXPRelease[]
[source,options="nowrap",subs="+attributes"]
----
$ helm repo add jboss-eap https://jbossas.github.io/eap-charts/
"jboss-eap" has been added to your repositories
$ helm search repo jboss-eap
NAME                    CHART VERSION   APP VERSION     DESCRIPTION
{helmChartName}         ...             ...             A Helm chart to build and deploy EAP XP {productVersion} applications
----
endif::[]

:leveloffset: 1

ifdef::helm-install-prerequisites-kubernetes[]
// Additional steps needed before deploying in Helm
[[deploy_helm_prerequisites]]
:leveloffset: +1

:prereq-kubernetes: true
ifdef::prereq-openshift[]
:prereq-cloud-platform: OpenShift
:prereq-cloud-cli: oc
:prereq-suffix: openshift
endif::[]
ifdef::prereq-kubernetes[]
:prereq-cloud-platform: Kubernetes
:prereq-cloud-cli: kubectl
:prereq-suffix: kubernetes
endif::[]

=== Install OpenTelemetry Collector on {prereq-cloud-platform}

The functionality of this quickstart depends on a running instance of the https://opentelemetry.io/docs/collector/[OpenTelemetry Collector].

To deploy and configure the OpenTelemetry Collector, you will need to apply a set of configurations to your {prereq-cloud-platform} cluster, to configure the OpenTelemetry Collector as well as any external routes needed:

[source,options="nowrap",subs="+attributes"]
----
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: collector-config
data:
  collector.yml: |
    receivers:
      otlp:
        protocols:
          grpc:
              endpoint: "0.0.0.0:4317"
          http:
              endpoint: "0.0.0.0:4318"
    processors:
    exporters:
      debug:
        verbosity: detailed
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: []
          exporters: [debug]
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetrycollector
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: opentelemetrycollector
  template:
    metadata:
      labels:
        app.kubernetes.io/name: opentelemetrycollector
    spec:
      containers:
        - name: otelcol
          args:
            - --config=/conf/collector.yml
          image: otel/opentelemetry-collector:0.89.0
          volumeMounts:
            - mountPath: /conf
              name: collector-config
      volumes:
        - configMap:
            items:
              - key: collector.yml
                path: collector.yml
            name: collector-config
          name: collector-config
---
apiVersion: v1
kind: Service
metadata:
  name: opentelemetrycollector
spec:
  ports:
    - name: otlp-grpc
      port: 4317
      protocol: TCP
      targetPort: 4317
    - name: otlp-http
      port: 4318
      protocol: TCP
      targetPort: 4318
  selector:
    app.kubernetes.io/name: opentelemetrycollector
  type: ClusterIP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: otelcol-otlp-grpc
  labels:
    app.kubernetes.io/name: microprofile
spec:
  port:
    targetPort: otlp-grpc
  to:
    kind: Service
    name: opentelemetrycollector
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: otelcol-otlp-http
  labels:
    app.kubernetes.io/name: microprofile
spec:
  port:
    targetPort: otlp-http
  to:
    kind: Service
    name: opentelemetrycollector
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None
----

To make things simpler, you can find these commands in `charts/opentelemetry-collector-openshift.yaml`, and to apply them run the following command in your terminal:

[source,options="nowrap",subs="+attributes"]
----
$ {prereq-cloud-cli} apply -f charts/opentelemetry-collector-{prereq-suffix}.yaml
----

[NOTE]
====
When done with the quickstart, the `{prereq-cloud-cli} delete -f charts/opentelemetry-collector-{prereq-suffix}.yaml` command may be used to revert the applied changes.
====




:leveloffset: 1
endif::helm-install-prerequisites-kubernetes[]

//Prepare Helm for Quickstart Deployment
:leveloffset: +1

ifndef::helmSetWildFlyArgumentPrefix[]
// For use with nested Helm charts
:helmSetWildFlyArgumentPrefix:
endif::[]
ifeval::[{useHelmChartDir} == true]
:helm_chart_values: charts
endif::[]
ifndef::useHelmChartDir[]
:helm_chart_values: -f charts/helm.yaml {helmChartName}
endif::[]
ifdef::kubernetes[]
:helm-set-build-enabled: --set {helmSetWildFlyArgumentPrefix}build.enabled=false
:helm-set-deploy-route-enabled: --set {helmSetWildFlyArgumentPrefix}deploy.route.enabled=false
:helm-set-image-name: --set {helmSetWildFlyArgumentPrefix}image.name="localhost:5000/{artifactId}"
:helm-extra-arguments: {helm-set-build-enabled} {helm-set-deploy-route-enabled} {helm-set-image-name}
:cloud-cli: kubectl
endif::[]
ifndef::kubernetes[]
:helm-extra-arguments:
:cloud-cli: oc
endif::[]
[[deploy_helm]]
== Deploy the {ProductShortName} Source-to-Image (S2I) Quickstart to {cloud-platform} with Helm Charts

ifndef::kubernetes[]
Log in to your OpenShift instance using the `oc login` command.
endif::[]
The backend will be built and deployed on {cloud-platform} with a Helm Chart for {productName}.


ifndef::kubernetes[]
Navigate to the root directory of this quickstart and run the following command:
endif::[]
ifdef::kubernetes[]
Navigate to the root directory of this quickstart and run the following commands:

[source,options="nowrap",subs="+attributes"]
----
mvn -Popenshift package wildfly:image
----
This will use the `openshift` Maven profile we saw earlier to build the application, and create a Docker image containing the {productName} server with the application deployed. The name of the image will be `{artifactId}`.

Next we need to tag the image and make it available to Kubernetes. You can push it to a registry like `quay.io`. In this case we tag as `localhost:5000/{artifactId}:latest` and push it to the internal registry in our Kubernetes instance:

[source,options="nowrap",subs="+attributes"]
----
# Tag the image
docker tag {artifactId} localhost:5000/{artifactId}:latest
# Push the image to the registry
docker push localhost:5000/{artifactId}:latest
----

In the below call to `helm install` which deploys our application to Kubernetes, we are passing in some extra arguments to tweak the Helm build:

* `{helm-set-build-enabled}` - This turns off the s2i build for the Helm chart since Kubernetes, unlike OpenShift, does not have s2i. Instead, we are providing the image to use.
* `{helm-set-deploy-route-enabled}` - This disables route creation normally performed by the Helm chart. On Kubernetes we will use port-forwards instead to access our application, since routes are an OpenShift specific concept and thus not available on Kubernetes.
* `{helm-set-image-name}` - This tells the Helm chart to use the image we built, tagged and pushed to Kubernetes' internal registry above.

endif::[]
[source,options="nowrap",subs="+attributes"]
----
$ helm install {helm-app-name} {helm_chart_values} --wait --timeout=10m0s {helm-extra-arguments}
NAME: {helm-app-name}
...
STATUS: deployed
REVISION: 1
----

This command will return once the application has successfully deployed. In case of a timeout, you can check the status of the application with the following command in another terminal:

[source,options="nowrap",subs="+attributes"]
----
{cloud-cli} get deployment {helm-app-name}
----

The Helm Chart for this quickstart contains all the information to build an image from the source code using S2I on Java 17:


ifndef::requires-http-route[]
ifdef::useHelmChartDir[]
[source,yaml]
----
include::{docdir}/charts/Chart.yaml[]
----
endif::useHelmChartDir[]
ifndef::useHelmChartDir[]
[source,yaml]
----
build:
  uri: https://github.com/wildfly/quickstart.git
  ref: main
  contextDir: opentelemetry-tracing
deploy:
  replicas: 1
  env:
    - name: OTEL_COLLECTOR_HOST
      value: "opentelemetrycollector"
----
endif::useHelmChartDir[]
endif::requires-http-route[]

ifdef::requires-http-route[]
[source,options="nowrap",subs="+attributes"]
----
build:
  uri: {githubRepoCodeUrl}
  ref: {WildFlyQuickStartRepoTag}
  contextDir: {artifactId}
deploy:
  replicas: 1
  route:
    tls:
      enabled: false
----
endif::requires-http-route[]

This will create a new deployment on {cloud-platform} and deploy the application.

If you want to see all the configuration elements to customize your deployment you can use the following command:
[source,options="nowrap",subs="+attributes"]
----
$ helm show readme {helmChartName}
----

ifdef::openshift[]
Get the URL of the route to the deployment.

[source,options="nowrap",subs="+attributes"]
----
$ oc get route {helm-app-name} -o jsonpath="{.spec.host}"
----
Access the application in your web browser using the displayed URL.
endif::[]
ifdef::kubernetes[]
To be able to connect to our application running in Kubernetes from outside, we need to set up a port-forward to the `{helm-app-name}` service created for us by the Helm chart.

This service will run on port `8080`, and we set up the port forward to also run on port `8080`:

[source,options="nowrap",subs="+attributes"]
----
kubectl port-forward service/{helm-app-name} 8080:8080
----
The server can now be accessed via `http://localhost:8080` from outside Kubernetes. Note that the command to create the port-forward will not return, so it is easiest to run this in a separate terminal.

endif::[]

ifdef::openshift+post-helm-install-actions-openshift[]
include::{post-helm-install-actions-openshift}[leveloffset=+1]
endif::openshift+post-helm-install-actions-openshift[]
ifdef::kubernetes+post-helm-install-actions-kubernetes[]
include::{post-helm-install-actions-kubernetes}[leveloffset=+1]
endif::kubernetes+post-helm-install-actions-kubernetes[]

:leveloffset: 1

// Testing on Openshift
:leveloffset: +1

[[run_the_integration_tests_with_kubernetes]]
= Run the Integration Tests with Kubernetes
The integration tests included with this quickstart, which verify that the quickstart runs correctly, may also be run with the quickstart running on Kubernetes.
[NOTE]
====
The integration tests expect a deployed application, so make sure you have deployed the quickstart on Kubernetes before you begin.
====

ifdef::extra-testing-actions-kubernetes[]
include::{extra-testing-actions-kubernetes}[leveloffset=+1]
endif::extra-testing-actions-kubernetes[]

ifndef::extra-test-arguments-kubernetes[:extra-test-arguments-kubernetes:]

Run the integration tests using the following command to run the `verify` goal with the `integration-testing` profile activated and the proper URL:
[source,options="nowrap",subs="+attributes"]
----
$ mvn verify -Pintegration-testing -Dserver.host=http://localhost:8080 {extra-test-arguments-kubernetes}
----


:leveloffset: 1

//Prepare Helm for Quickstart Deployment
:leveloffset: +1

[[undeploy_helm]]
== Undeploy the {ProductShortName} Source-to-Image (S2I) Quickstart from {cloud-platform} with Helm Charts

[source,options="nowrap",subs="+attributes"]
----
$ helm uninstall {helm-app-name}
----
ifdef::kubernetes[]
To stop the port forward you created earlier use:
[source,options="nowrap",subs="+attributes"]
----
$ kubectl port-forward service/{helm-app-name} 8080:8080
----
endif::[]

:leveloffset: 1

// Unset the attribute
:!kubernetes:

:leveloffset!:
endif::[]

== Conclusion

OpenTelemetry Tracing provides the mechanisms for your application to participate
in the distributed tracing with minimal effort on the application side. The Jakarta REST
resources are always traced by default, but the specification allows you to create
individual spans directly with the CDI injection of the `io.opentelemetry.api.trace.Tracer`.
