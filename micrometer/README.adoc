ifdef::env-github[]
:artifactId: micrometer
endif::[]

//***********************************************************************************
// Enable the following flag to build README.html files for JBoss EAP product builds.
// Comment it out for WildFly builds.
//***********************************************************************************
//:ProductRelease:

//***********************************************************************************
// Enable the following flag to build README.html files for EAP XP product builds.
// Comment it out for WildFly or JBoss EAP product builds.
//***********************************************************************************
//:EAPXPRelease:

// This is a universal name for all releases
:ProductShortName: JBoss EAP
// Product names and links are dependent on whether it is a product release (CD or JBoss)
// or the WildFly project.
// The "DocInfo*" attributes are used to build the book links to the product documentation

ifdef::ProductRelease[]
// JBoss EAP release
:productName: JBoss EAP
:productNameFull: Red Hat JBoss Enterprise Application Platform
:productVersion: 8.0
:DocInfoProductNumber: {productVersion}
:WildFlyQuickStartRepoTag: 8.0.x
:helmChartName: jboss-eap/eap8
endif::[]

ifdef::EAPXPRelease[]
// JBoss EAP XP release
:productName: JBoss EAP XP
:productNameFull: Red Hat JBoss Enterprise Application Platform expansion pack
:productVersion: 5.0
:WildFlyQuickStartRepoTag: XP_5.0.0.GA
endif::[]

ifdef::ProductRelease,EAPXPRelease[]
:githubRepoUrl: https://github.com/jboss-developer/jboss-eap-quickstarts/
:githubRepoCodeUrl: https://github.com/jboss-developer/jboss-eap-quickstarts.git
:jbossHomeName: EAP_HOME
:DocInfoProductName: Red Hat JBoss Enterprise Application Platform
:DocInfoProductNameURL: red_hat_jboss_enterprise_application_platform
:DocInfoPreviousProductName: jboss-enterprise-application-platform
:quickstartDownloadName: {productNameFull} {productVersion} Quickstarts
:quickstartDownloadUrl: https://access.redhat.com/jbossnetwork/restricted/listSoftware.html?product=appplatform&downloadType=distributions
:helmRepoName: jboss-eap
:helmRepoUrl: https://jbossas.github.io/eap-charts/
// END ifdef::ProductRelease,EAPXPRelease[]
endif::[]

ifndef::ProductRelease,EAPXPRelease[]
// WildFly project
:productName: WildFly
:productNameFull: WildFly Application Server
:ProductShortName: {productName}
:jbossHomeName: WILDFLY_HOME
:productVersion: 36
:githubRepoUrl: https://github.com/wildfly/quickstart/
:githubRepoCodeUrl: https://github.com/wildfly/quickstart.git
:WildFlyQuickStartRepoTag: 36.0.1.Final
:DocInfoProductName: Red Hat JBoss Enterprise Application Platform
:DocInfoProductNameURL: red_hat_jboss_enterprise_application_platform
:DocInfoPreviousProductName: jboss-enterprise-application-platform
:helmRepoName: wildfly
:helmRepoUrl: http://docs.wildfly.org/wildfly-charts/
:helmChartName: wildfly/wildfly
// END ifndef::ProductRelease,EAPCDRelease,EAPXPRelease[]
endif::[]

:source: {githubRepoUrl}

// Values for Openshift S2i sections attributes
:EapForOpenshiftBookName: {productNameFull} for OpenShift
:EapForOpenshiftOnlineBookName: {EapForOpenshiftBookName} Online
:xpaasproduct: {productNameFull} for OpenShift
:xpaasproduct-shortname: {ProductShortName} for OpenShift
:ContainerRegistryName: Red Hat Container Registry
:EapForOpenshiftBookName: Getting Started with {ProductShortName} for OpenShift Container Platform
:EapForOpenshiftOnlineBookName: Getting Started with {ProductShortName} for OpenShift Online
:OpenShiftOnlinePlatformName: Red Hat OpenShift Container Platform
:OpenShiftOnlineName: Red Hat OpenShift Online
:ImageandTemplateImportBaseURL: https://raw.githubusercontent.com/jboss-container-images/jboss-eap-openshift-templates
:ImageandTemplateImportURL: {ImageandTemplateImportBaseURL}/{ImagePrefixVersion}/
:BuildImageStream: jboss-{ImagePrefixVersion}-openjdk11-openshift
:RuntimeImageStream: jboss-{ImagePrefixVersion}-openjdk11-runtime-openshift

// Links to the OpenShift documentation
:LinkOpenShiftGuide: https://access.redhat.com/documentation/en-us/{DocInfoProductNameURL}/{DocInfoProductNumber}/html-single/getting_started_with_jboss_eap_for_openshift_container_platform/
:LinkOpenShiftOnlineGuide: https://access.redhat.com/documentation/en-us/{DocInfoProductNameURL}/{DocInfoProductNumber}/html-single/getting_started_with_jboss_eap_for_openshift_online/

ifdef::EAPXPRelease[]
// Attributes for XP releases
:EapForOpenshiftBookName: {productNameFull} for OpenShift
:EapForOpenshiftOnlineBookName: {productNameFull} for OpenShift Online
:xpaasproduct: {productNameFull} for OpenShift
:ContainerRegistryName: Red Hat Container Registry
:EapForOpenshiftBookName: {productNameFull} for OpenShift
:EapForOpenshiftOnlineBookName: {productNameFull} for OpenShift Online
:ImageandTemplateImportURL: {ImageandTemplateImportBaseURL}/{ImagePrefixVersion}/
:BuildImageStream: jboss-{ImagePrefixVersion}-openjdk11-openshift
:RuntimeImageStream: jboss-{ImagePrefixVersion}-openjdk11-runtime-openshift
// Links to the OpenShift documentation
:LinkOpenShiftGuide: https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/{DocInfoProductNumber}/html/using_eclipse_microprofile_in_jboss_eap/using-the-openshift-image-for-jboss-eap-xp_default
:LinkOpenShiftOnlineGuide: https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/{DocInfoProductNumber}/html/using_eclipse_microprofile_in_jboss_eap/using-the-openshift-image-for-jboss-eap-xp_default
endif::[]

ifndef::ProductRelease,EAPCDRelease,EAPXPRelease[]
:ImageandTemplateImportURL: https://raw.githubusercontent.com/wildfly/wildfly-s2i/v{productVersion}.0/
endif::[]

//*************************
// Other values
//*************************
:buildRequirements: Java SE 17.0 or later, and Maven 3.6.0 or later
:javaVersion: Jakarta EE 10
ifdef::EAPXPRelease[]
:javaVersion: Eclipse MicroProfile
endif::[]
:guidesBaseUrl: https://github.com/jboss-developer/jboss-developer-shared-resources/blob/master/guides/
:useEclipseUrl: {guidesBaseUrl}USE_JBDS.adoc#use_red_hat_jboss_developer_studio_or_eclipse_to_run_the_quickstarts
:useEclipseDeployJavaClientDocUrl: {guidesBaseUrl}USE_JBDS.adoc#deploy_and_undeploy_a_quickstart_containing_server_and_java_client_projects
:useEclipseDeployEARDocUrl: {guidesBaseUrl}USE_JBDS.adoc#deploy_and_undeploy_a_quickstart_ear_project
:useProductHomeDocUrl: {guidesBaseUrl}USE_OF_{jbossHomeName}.adoc#use_of_product_home_and_jboss_home_variables
:configureMavenDocUrl: {guidesBaseUrl}CONFIGURE_MAVEN_JBOSS_EAP.adoc#configure_maven_to_build_and_deploy_the_quickstarts
:addUserDocUrl: {guidesBaseUrl}CREATE_USERS.adoc#create_users_required_by_the_quickstarts
:addApplicationUserDocUrl: {guidesBaseUrl}CREATE_USERS.adoc#add_an_application_user
:addManagementUserDocUrl: {guidesBaseUrl}CREATE_USERS.adoc#add_an_management_user
:startServerDocUrl: {guidesBaseUrl}START_JBOSS_EAP.adoc#start_the_jboss_eap_server
:configurePostgresDocUrl: {guidesBaseUrl}CONFIGURE_POSTGRESQL_JBOSS_EAP.adoc#configure_the_postgresql_database_for_use_with_the_quickstarts
:configurePostgresDownloadDocUrl: {guidesBaseUrl}CONFIGURE_POSTGRESQL_JBOSS_EAP.adoc#download_and_install_postgresql
:configurePostgresCreateUserDocUrl: {guidesBaseUrl}CONFIGURE_POSTGRESQL_JBOSS_EAP.adoc#create_a_database_user
:configurePostgresAddModuleDocUrl: {guidesBaseUrl}CONFIGURE_POSTGRESQL_JBOSS_EAP.adoc#add_the_postgres_module_to_the_jboss_eap_server
:configurePostgresDriverDocUrl: {guidesBaseUrl}CONFIGURE_POSTGRESQL_JBOSS_EAP.adoc#configure_the_postgresql_driver_in_the_jboss_eap_server
:configureBytemanDownloadDocUrl: {guidesBaseUrl}CONFIGURE_BYTEMAN.adoc#download_and_configure_byteman
:configureBytemanDisableDocUrl: {guidesBaseUrl}CONFIGURE_BYTEMAN.adoc#disable_the_byteman_script
:configureBytemanClearDocUrl: {guidesBaseUrl}CONFIGURE_BYTEMAN.adoc#clear_the_transaction_object_store
:configureBytemanQuickstartDocUrl: {guidesBaseUrl}CONFIGURE_BYTEMAN.adoc#configure_byteman_for_use_with_the_quickstarts
:configureBytemanHaltDocUrl: {guidesBaseUrl}CONFIGURE_BYTEMAN.adoc#use_byteman_to_halt_the_application[
:configureBytemanQuickstartsDocUrl: {guidesBaseUrl}CONFIGURE_BYTEMAN.adoc#configure_byteman_for_use_with_the_quickstarts

= micrometer: Micrometer QuickStart
:author: Jason Lee
:level: Beginner
:technologies: Micrometer
:openshift: true

[abstract]
The `micrometer` quickstart demonstrates the use of the Micrometer library in {productName}.

:standalone-server-type: default
:archiveType: war
:archiveName: {artifactId}
:restoreScriptName: restore-configuration.cli
:helm-install-prerequisites-openshift: ../micrometer/helm-install-prerequisites-openshift.adoc
:helm-install-prerequisites-kubernetes: ../micrometer/helm-install-prerequisites-kubernetes.adoc

== What is it?

https://micrometer.io[Micrometer] is a vendor-neutral facade that allows application developers to collect and report application and system metrics to the backend of their choice in an entirely portable manner. By simply replacing the `MeterRegistry` used, or combining them in Micrometer's `CompositeRegistry` data can be exported a variety of monitoring systems with no application code changes.

== Architecture

In this quickstart, we will build a small, simple application that shows the usage of a number of Micrometer's `Meter` implementations. We will also demonstrate the means by which {productName} exports the metrics data, which is via the https://opentelemetry.io/docs/reference/specification/protocol/otlp/[OpenTelemetry Protocol (OTLP)] to the https://opentelemetry.io/docs/collector/[OpenTelemetry Collector]. To provide simpler access to the published metrics, the Collector will be configured with a Prometheus endpoint, from which we can scrape data.

== Prerequisites

To complete this guide, you will need:

* less than 15 minutes
* JDK 11+ installed with `JAVA_HOME` configured appropriately
* Apache Maven 3.5.3+
* Docker Compose, or alternatively Podman Compose

:leveloffset: +1

ifdef::requires-multiple-servers[]
[[use_of_jboss_home_name]]
= Use of the {jbossHomeName}_1, {jbossHomeName}_2, and QUICKSTART_HOME Variables

This quickstart requires that you clone your `__{jbossHomeName}__` installation directory and run two servers. The installation path is described in detail here: link:{useProductHomeDocUrl}[Use of __{jbossHomeName}__ and __JBOSS_HOME__ Variables].

In the following instructions, replace `__{jbossHomeName}_1__` with the path to your first {productName} server and replace `__{jbossHomeName}_2__` with the path to your second cloned {productName} server.

When you see the replaceable variable __QUICKSTART_HOME__, replace it with the path to the root directory of all of the quickstarts.
endif::[]

ifdef::optional-domain-or-multiple-servers[]
[[use_of_jboss_home_name]]
= Use of the {jbossHomeName}_1, {jbossHomeName}_2, and QUICKSTART_HOME Variables

When deploying this quickstart to a managed domain, replace `__{jbossHomeName}__` with the actual path to your {productName} installation. The installation path is described in detail here: link:{useProductHomeDocUrl}[Use of __{jbossHomeName}__ and __JBOSS_HOME__ Variables].

When deploying this quickstart to multiple standalone servers, this quickstart requires that you clone your `__{jbossHomeName}__` installation directory and run two servers. In the following instructions, replace `__{jbossHomeName}_1__` with the path to your first {productName} server and replace `__{jbossHomeName}_2__` with the path to your second cloned {productName} server.

When you see the replaceable variable __QUICKSTART_HOME__, replace it with the path to the root directory of all of the quickstarts.
endif::[]

ifndef::requires-multiple-servers,optional-domain-or-multiple-servers[]
[[use_of_jboss_home_name]]
= Use of the {jbossHomeName} and QUICKSTART_HOME Variables

In the following instructions, replace `__{jbossHomeName}__` with the actual path to your {productName} installation. The installation path is described in detail here: link:{useProductHomeDocUrl}[Use of __{jbossHomeName}__ and __JBOSS_HOME__ Variables].

When you see the replaceable variable __QUICKSTART_HOME__, replace it with the path to the root directory of all of the quickstarts.
endif::[]

:leveloffset!:

== Steps

// Start the {productName} Standalone Server
:leveloffset: +1

[[start_the_eap_standalone_server]]
= Start the {productName} Standalone Server
//******************************************************************************
// Include this template if your quickstart requires a normal start of a single
// standalone server.
//
// You must define the `standalone-server-type`. Supported values are:
//    default
//    full
//    full-ha
//    ha
//    custom
//
// * For mobile applications, you can define the `mobileApp` variable in the
//   `README.adoc` file to add `-b 0.0.0.0` to the command line. This allows
//    external clients, such as phones, tablets, and desktops, to connect
//    to the application through through your local network
//    ::mobileApp: {artifactId}-service
//
//******************************************************************************

//******************************************************************************
// This template sets attributes for the different standalone server profiles.
//
// You must define the `standalone-server-type`. Supported values are:
//    default
//    full
//    full-ha
//    ha
//    microprofile
//    custom
//******************************************************************************

// Standalone server with the default profile.
ifeval::["{standalone-server-type}"=="default"]
:serverProfile: default profile
:configFileName: standalone/configuration/standalone.xml
:serverArguments:
endif::[]

// Standalone server with the full profile.
ifeval::["{standalone-server-type}"=="full"]
:serverProfile: full profile
:configFileName: standalone/configuration/standalone-full.xml
:serverArguments:  -c standalone-full.xml
endif::[]

// Standalone server with the full HA profile.
ifeval::["{standalone-server-type}"=="full-ha"]
:serverProfile: full HA profile
:configFileName: standalone/configuration/standalone-full-ha.xml
:serverArguments:  -c standalone-full-ha.xml
endif::[]

// Start the standalone server with the HA profile.
ifeval::["{standalone-server-type}"=="ha"]
:serverProfile: HA profile
:configFileName: standalone/configuration/standalone-ha.xml
:serverArguments:  -c standalone-ha.xml
endif::[]

// Start the standalone server with the Eclipse MicroProfile profile.
ifeval::["{standalone-server-type}"=="microprofile"]
:serverProfile: MicroProfile profile
:configFileName: standalone/configuration/standalone-microprofile.xml
:serverArguments:  -c standalone-microprofile.xml
endif::[]

// Standalone server with the custom profile.
// NOTE: This profile requires that you define the `serverArguments` variable
// within the quickstart README.adoc file. For example:
//  :serverArguments: --server-config=../../docs/examples/configs/standalone-xts.xml
ifeval::["{standalone-server-type}"=="custom"]
:serverProfile: custom profile
endif::[]

// If there is no match, use the default profile.
ifndef::serverProfile[]
:standalone-server-type:  default
:serverProfile: default profile
:configFileName: standalone/configuration/standalone.xml
:serverArguments:
endif::serverProfile[]

. Open a terminal and navigate to the root of the {productName} directory.
. Start the {productName} server with the {serverProfile} by typing the following command.
+
ifdef::uses-jaeger[]
[source,subs="+quotes,attributes+",options="nowrap"]
----
$ __JAEGER_REPORTER_LOG_SPANS=true JAEGER_SAMPLER_TYPE=const JAEGER_SAMPLER_PARAM=1__ __{jbossHomeName}__/bin/standalone.sh {serverArguments}
----
endif::[]
ifndef::uses-jaeger[]
[source,subs="+quotes,attributes+",options="nowrap"]
----
$ __{jbossHomeName}__/bin/standalone.sh {serverArguments}
----
endif::[]
+
NOTE: For Windows, use the `__{jbossHomeName}__\bin\standalone.bat` script.

ifdef::mobileApp[]
+
Adding `-b 0.0.0.0` to the above command allows external clients, such as phones, tablets, and desktops, to connect through your local network. For example:
+
[source,subs="+quotes,attributes+",options="nowrap"]
----
$ __{jbossHomeName}__/bin/standalone.sh {serverArguments} -b 0.0.0.0
----
endif::[]

:leveloffset!:

[[configure_the_server]]
=== Configure the Server

You enable Micrometer by running JBoss CLI commands. For your convenience, this quickstart batches the commands into a `configure-micrometer.cli` script provided in the root directory of this quickstart.

. Before you begin, make sure you do the following:

* xref:back_up_standalone_server_configuration[Back up the {productName} standalone server configuration] as described above.
* xref:start_the_eap_standalone_server[Start the {productName} server with the standalone default profile] as described above.

. Review the `configure-micrometer.cli` file in the root of this quickstart directory. This script adds the configuration that enables Micrometer for the quickstart components. Comments in the script describe the purpose of each block of commands.
. Open a new terminal, navigate to the root directory of this quickstart, and run the following command, replacing `__{jbossHomeName}__` with the path to your server:
+
[source,subs="+quotes,attributes+",options="nowrap"]
----
$ __{jbossHomeName}__/bin/jboss-cli.sh --connect --file=configure-micrometer.cli
----
+
NOTE: For Windows, use the `__{jbossHomeName}__\bin\jboss-cli.bat` script.
+

You should see the following result when you run the script:
+
[source,options="nowrap"]
----
The batch executed successfully
process-state: reload-required
----

. You'll need to reload the configuration after that:
+
[source,subs="+quotes,attributes+",options="nowrap"]
----
$ __{jbossHomeName}__/bin/jboss-cli.sh --connect --commands=reload
----

[[start_opentelemetry_collector]]
=== Starting the OpenTelemetry Collector

By default, {productName} will publish metrics every 10 seconds, so you will soon start seeing errors about a refused connection.

This is because we told {productName} to publish to a server that is not there, so we need to fix that. To make that as simple as possible, you can use Docker Compose to start an instance of the OpenTelemetry Collector.

The Docker Compose configuration file is `docker-compose.yaml`:

[source,yaml]
----
version: "3"

services:
  otel-collector:
    image: otel/opentelemetry-collector:0.115.1
    command: [--config=/etc/otel-collector-config.yaml]
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml:Z
    ports:
      - 1888:1888 # pprof extension
      - 8888:8888 # Prometheus metrics exposed by the collector
      - 8889:8889 # Prometheus exporter metrics
      - 13133:13133 # health_check extension
      - 4317:4317 # OTLP gRPC receiver
      - 4318:4318 # OTLP http receiver
      - 55679:55679 # zpages extension
      - 1234:1234 # /metrics endpoint
----

The Collector server configuration file is `otel-collector-config.yaml`:

[source,yaml]
----
extensions:
  health_check:
  pprof:
    endpoint: 0.0.0.0:1777
  zpages:
    endpoint: 0.0.0.0:55679

receivers:
  otlp:
    protocols:
      grpc:
          endpoint: "0.0.0.0:4317"
      http:
          endpoint: "0.0.0.0:4318"

processors:
  batch:

exporters:
  debug:
    verbosity: detailed
  prometheus:
    endpoint: "0.0.0.0:1234"

service:
  pipelines:
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [debug,prometheus]

  extensions: [health_check, pprof, zpages]

----

We can now bring up the collector server instance:

[source,bash]
----
$ docker-compose up
----

The service should be available almost immediately, which you can verify by looking at the Prometheus endpoint we've configured by pointing your browser at http://localhost:1234/metrics[]. You should see quite a few metrics listed, none of which are what our application has registered. What you're seeing are the system and JVM metrics automatically registered and published by {productName} to give systems/applications administrators a comprehensive view of system health and performance.

[NOTE]
====
You may use Podman as alternative to Docker if you prefer, in such case the command should be `podman-compose up`.
====

[NOTE]
====
If your environment does not support Docker or Podman, please refer to https://opentelemetry.io/docs/collector/installation[Otel Collector documentation] for alternatives on how to install and run the OpenTelemetry Collector. Please ensure the same OpenTelemetry version as the one in the docker-compose.yaml above is used, otherwise such configuration may fail to work.
====

=== Registering metrics

Micrometer uses a programmatic approach to metrics definition, as opposed the more declarative, annotation-based approach of other libraries. Because of that, we need to explicitly register our `Meter` s before they can be used:

[source,java]
----
@Path("/")
@ApplicationScoped
public class RootResource {
    // ...
    @Inject
    private MeterRegistry registry;

    private Counter performCheckCounter;
    private Counter originalCounter;
    private Counter duplicatedCounter;

    @PostConstruct
    private void createMeters() {
        Gauge.builder("prime.highestSoFar", () -> highestPrimeNumberSoFar)
                .description("Highest prime number so far.")
                .register(registry);
        performCheckCounter = Counter
                .builder("prime.performedChecks")
                .description("How many prime checks have been performed.")
                .register(registry);
        originalCounter = Counter
                .builder("prime.duplicatedCounter")
                .tags(List.of(Tag.of("type", "original")))
                .register(registry);
        duplicatedCounter = Counter
                .builder("prime.duplicatedCounter")
                .tags(List.of(Tag.of("type", "copy")))
                .register(registry);
    }
    // ...
}
----

Notice that we start by `@Inject` ing the `MeterRegistry`. This is a {productName}-managed instance, so all applications need to do it inject it and start using. Once we have that, we can use to build and register our meters, which we do in `@PostConstuct private void createMeters()`

[NOTE]
====
This must be done _post_-construction, as the `MeterRegistry` must be injected before it can be used to register the meters.
====

In this example, we register several different types to demonstrate their use. With those registered, we can start writing application logic:

[source,java]
----
@GET
@Path("/prime/{number}")
public String checkIfPrime(@PathParam("number") long number) throws Exception {
    performCheckCounter.increment();

    Timer timer = registry.timer("prime.timer");

    return timer.recordCallable(() -> {

        if (number < 1) {
            return "Only natural numbers can be prime numbers.";
        }

        if (number == 1) {
            return "1 is not prime.";
        }

        if (number == 2) {
            return "2 is prime.";
        }

        if (number % 2 == 0) {
            return number + " is not prime, it is divisible by 2.";
        }

        for (int i = 3; i < Math.floor(Math.sqrt(number)) + 1; i = i + 2) {
            try {
                Thread.sleep(10);
            } catch (InterruptedException e) {
                //
            }
            if (number % i == 0) {
                return number + " is not prime, is divisible by " + i + ".";
            }
        }

        if (number > highestPrimeNumberSoFar) {
            highestPrimeNumberSoFar = number;
        }

        return number + " is prime.";
    });
}
----

This method represents a simple REST endpoint that is able to determine whether the number passed as a path parameter is a prime number.

:leveloffset: +1

[[build_and_deploy_the_quickstart]]
= Build and Deploy the Quickstart
//******************************************************************************
// Include this template if your quickstart does a normal deployment of a archive.
//
// * Define the `archiveType` variable in the quickstart README file.
//   Supported values:
//    :archiveType: ear
//    :archiveType: war
//    :archiveType: jar
//
// * To override the archive name, which defaults to the {artifactId),
//   define the `archiveName` variable, for example:
//    :archiveName: {artifactId}-service
//
// * To override the archive output directory,
//   define the `archiveDir` variable, for example:
//    :archiveDir: ear/target
//
// * To override the Maven command, define the `mavenCommand` variable,
//   for example:
//    :mavenCommand: clean install wildfly:deploy
//******************************************************************************

// The archive name defaults to the artifactId if not overridden
ifndef::archiveName[]
:archiveName: {artifactId}
endif::archiveName[]

// The archive type defaults to war if not overridden
ifndef::archiveType[]
:archiveType: war
endif::archiveType[]

// Define the archive file name as the concatenation of "archiveName" + "." + "archiveType+
:archiveFileName: {archiveName}.{archiveType}

// If they have not defined the target archive directory, make it the default for the archive type.
ifndef::archiveDir[]

ifeval::["{archiveType}"=="ear"]
:archiveDir: {artifactId}/ear/target
endif::[]

ifeval::["{archiveType}"=="war"]
:archiveDir: {artifactId}/target
endif::[]

ifeval::["{archiveType}"=="jar"]
:archiveDir: {artifactId}/target
endif::[]

endif::archiveDir[]

ifndef::mavenCommand[]
ifeval::["{archiveType}"=="ear"]
:mavenCommand: clean install
endif::[]

ifeval::["{archiveType}"=="war"]
:mavenCommand: clean package
endif::[]

ifeval::["{archiveType}"=="jar"]
:mavenCommand: clean install
endif::[]

endif::mavenCommand[]

. Make sure {productName} server is started.
. Open a terminal and navigate to the root directory of this quickstart.
ifdef::reactive-messaging[]
. Run this command to enable the MicroProfile Reactive Messaging functionality on the server
+
[source,subs="attributes+",options="nowrap"]
----
$ __{jbossHomeName}__/bin/jboss-cli.sh --connect --file=enable-reactive-messaging.cli
----
endif::reactive-messaging[]
. Type the following command to build the quickstart.
+
[source,subs="attributes+",options="nowrap"]
----
$ mvn {mavenCommand}
----

. Type the following command to deploy the quickstart.
+
[source,subs="attributes+",options="nowrap"]
----
$ mvn wildfly:deploy
----

This deploys the `{archiveDir}/{archiveFileName}` to the running instance of the server.

You should see a message in the server log indicating that the archive deployed successfully.

:leveloffset!:

=== Access the quickstart application

You can either access the application via your browser at http://localhost:8080/micrometer/prime/13[], or from the command line:

[source,bash]
----
$ curl http://localhost:8080/micrometer/prime/13
----

It should return a simple document:

[source]
-----
13 is prime.
-----

Once given enough time to allow {productName} to publish metrics updates, you now see your application's meters reported in the http://localhost:1234/metrics[Prometheus export]. You can also view them via the command-line:

[source,bash]
----
$ curl -s http://localhost:1234/metrics | grep "prime_"
# HELP prime_duplicatedCounter
# TYPE prime_duplicatedCounter counter
prime_duplicatedCounter{job="wildfly",type="copy"} 0
prime_duplicatedCounter{job="wildfly",type="original"} 0
# HELP prime_highestSoFar Highest prime number so far.
# TYPE prime_highestSoFar gauge
prime_highestSoFar{job="wildfly"} 13
# HELP prime_performedChecks How many prime checks have been performed.
# TYPE prime_performedChecks counter
prime_performedChecks{job="wildfly"} 1
# HELP prime_timer
# TYPE prime_timer histogram
prime_timer_bucket{job="wildfly",le="+Inf"} 1
prime_timer_sum{job="wildfly"} 10.941035
prime_timer_count{job="wildfly"} 1

----

Notice that all four meters registered in the `@PostConstruct` method as well as the `Timer` in our endpoint method have all been published.

// Server Distribution Testing
:leveloffset: +2

[[run_the_integration_tests_with_server_distribution]]
= Run the Integration Tests
ifndef::integrationTestsDirectory[:integrationTestsDirectory: src/test/]
ifndef::extraStandardDistTestParams[:extraStandardDistTestParams: ]

This quickstart includes integration tests, which are located under the `{integrationTestsDirectory}` directory. The integration tests verify that the quickstart runs correctly when deployed on the server.

Follow these steps to run the integration tests.

. Make sure {productName} server is started.
. Make sure the quickstart is deployed.
. Type the following command to run the `verify` goal with the `integration-testing` profile activated.
+
[source,subs="attributes+",options="nowrap"]
----
$ mvn verify -Pintegration-testing {extraStandardDistTestParams}
----

:leveloffset!:
// Undeploy the Quickstart
:leveloffset: +2

[[undeploy_the_quickstart]]
= Undeploy the Quickstart

//*******************************************************************************
// Include this template if your quickstart does a normal undeployment of an archive.
//*******************************************************************************
When you are finished testing the quickstart, follow these steps to undeploy the archive.

. Make sure {productName} server is started.
. Open a terminal and navigate to the root directory of this quickstart.
. Type this command to undeploy the archive:
+
[source,options="nowrap"]
----
$ mvn wildfly:undeploy
----

:leveloffset!:
//  Restore the {productName} Standalone Server Configuration
:leveloffset: +2

[[restore_the_standalone_server_configuration]]
= Restore the {productName} Standalone Server Configuration
//******************************************************************************
// Include this template if your quickstart does a normal restoration of a single
// standalone server configuration.
// * It provides a CLI script.
// * You can manually restore the backup copy.
//
// You must define the script file name using the `restoreScriptName` attribute.
// For example:
// :restoreScriptName: remove-configuration.cli
//******************************************************************************

You can restore the original server configuration using either of the following methods.

* You can xref:restore_standalone_server_configuration_using_cli[run the `{restoreScriptName}` script] provided in the root directory of this quickstart.
* You can xref:restore_standalone_server_configuration_manually[manually restore the configuration] using the backup copy of the configuration file.

[[restore_standalone_server_configuration_using_cli]]
== Restore the {productName} Standalone Server Configuration by Running the JBoss CLI Script

. xref:start_the_eap_standalone_server[Start the {productName} server] as described above.
. Open a new terminal, navigate to the root directory of this quickstart, and run the following command, replacing `__{jbossHomeName}__` with the path to your server:
+
[source,subs="+quotes,attributes+",options="nowrap"]
----
$ __{jbossHomeName}__/bin/jboss-cli.sh --connect --file={restoreScriptName}
----
+
NOTE: For Windows, use the `__{jbossHomeName}__\bin\jboss-cli.bat` script.

:leveloffset!:
// Restore the {productName} Standalone Server Configuration Manually
:leveloffset: +3

[[restore_standalone_server_configuration_manually]]
= Restore the {productName} Standalone Server Configuration Manually
//******************************************************************************
// Include this template if your quickstart does a normal manual restoration
// of a single standalone server.
//******************************************************************************

//******************************************************************************
// This template sets attributes for the different standalone server profiles.
//
// You must define the `standalone-server-type`. Supported values are:
//    default
//    full
//    full-ha
//    ha
//    microprofile
//    custom
//******************************************************************************

// Standalone server with the default profile.
ifeval::["{standalone-server-type}"=="default"]
:serverProfile: default profile
:configFileName: standalone/configuration/standalone.xml
:serverArguments:
endif::[]

// Standalone server with the full profile.
ifeval::["{standalone-server-type}"=="full"]
:serverProfile: full profile
:configFileName: standalone/configuration/standalone-full.xml
:serverArguments:  -c standalone-full.xml
endif::[]

// Standalone server with the full HA profile.
ifeval::["{standalone-server-type}"=="full-ha"]
:serverProfile: full HA profile
:configFileName: standalone/configuration/standalone-full-ha.xml
:serverArguments:  -c standalone-full-ha.xml
endif::[]

// Start the standalone server with the HA profile.
ifeval::["{standalone-server-type}"=="ha"]
:serverProfile: HA profile
:configFileName: standalone/configuration/standalone-ha.xml
:serverArguments:  -c standalone-ha.xml
endif::[]

// Start the standalone server with the Eclipse MicroProfile profile.
ifeval::["{standalone-server-type}"=="microprofile"]
:serverProfile: MicroProfile profile
:configFileName: standalone/configuration/standalone-microprofile.xml
:serverArguments:  -c standalone-microprofile.xml
endif::[]

// Standalone server with the custom profile.
// NOTE: This profile requires that you define the `serverArguments` variable
// within the quickstart README.adoc file. For example:
//  :serverArguments: --server-config=../../docs/examples/configs/standalone-xts.xml
ifeval::["{standalone-server-type}"=="custom"]
:serverProfile: custom profile
endif::[]

// If there is no match, use the default profile.
ifndef::serverProfile[]
:standalone-server-type:  default
:serverProfile: default profile
:configFileName: standalone/configuration/standalone.xml
:serverArguments:
endif::serverProfile[]

When you have completed testing the quickstart, you can restore the original server configuration by manually restoring the backup copy the configuration file.

. If it is running, stop the {productName} server.
. Replace the `__{jbossHomeName}__/{configFileName}` file with the backup copy of the file.

:leveloffset!:
// Build and run sections for other environments/builds
:leveloffset: +1

[[build_and_run_the_quickstart_with_provisioned_server]]
= Building and running the quickstart application with provisioned {productName} server

ifndef::mavenServerProvisioningCommand[]
ifeval::["{archiveType}"=="ear"]
:mavenServerProvisioningCommand: clean install
endif::[]
ifeval::["{archiveType}"=="war"]
:mavenServerProvisioningCommand: clean package
endif::[]
ifeval::["{archiveType}"=="jar"]
:mavenServerProvisioningCommand: clean install
endif::[]
endif::mavenServerProvisioningCommand[]

ifndef::deploymentTargetDir[]
ifndef::deploymentDir[:deploymentTargetDir: target]
ifdef::deploymentDir[:deploymentTargetDir: {deploymentDir}/target]
endif::deploymentTargetDir[]

ifndef::extraStartParams[:extraStartParams: ]
ifndef::extraProvisioningTestParams[:extraProvisioningTestParams: ]

Instead of using a standard {productName} server distribution, you can alternatively provision a {productName} server to deploy and run the quickstart. The functionality is provided by the WildFly Maven Plugin, and you may find its configuration in the quickstart `pom.xml`:

ifndef::ProductRelease[]
[source,xml,subs="attributes+"]
----
<profile>
    <id>provisioned-server</id>
    <activation>
        <activeByDefault>true</activeByDefault>
    </activation>
    <build>
        <plugins>
            <plugin>
                <groupId>org.wildfly.plugins</groupId>
                <artifactId>wildfly-maven-plugin</artifactId>
                <configuration>
                    <discover-provisioning-info>
                        <version>${version.server}</version>
                    </discover-provisioning-info>
                    <add-ons>...</add-ons>
                </configuration>
                <executions>
                    <execution>
                        <goals>
                            <goal>package</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            ...
        </plugins>
    </build>
</profile>
----
endif::[]

ifdef::ProductRelease[]
[source,xml,subs="attributes+"]
----
<profile>
    <id>provisioned-server</id>
    <activation>
        <activeByDefault>true</activeByDefault>
    </activation>
    <build>
        <plugins>
            <plugin>
                <groupId>org.jboss.eap.plugins</groupId>
                <artifactId>eap-maven-plugin</artifactId>
                <configuration>
                    ...
                    <feature-packs>
                        <feature-pack>
                            <location>org.jboss.eap:wildfly-ee-galleon-pack</location>
                        </feature-pack>
                        ...
                    </feature-packs>
                    <layers>...</layers>
                </configuration>
                <executions>
                    <execution>
                        <goals>
                            <goal>package</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            ...
        </plugins>
    </build>
</profile>
----
endif::[]

[NOTE]
====
When built, the provisioned {productName} server can be found in the `{deploymentTargetDir}/server` directory, and its usage is similar to a standard server distribution, with the simplification that there is never the need to specify the server configuration to be started.
====

Follow these steps to run the quickstart using the provisioned server.

.Procedure

. Make sure the server is provisioned.
+
[source,subs="attributes+",options="nowrap"]
----
$ mvn {mavenServerProvisioningCommand}
----

ifdef::addQuickstartUser[]
. Add the quickstart user:
+
[source,subs="+quotes,attributes+",options="nowrap"]
----
$ {deploymentTargetDir}/server/bin/add-user.sh -a -u 'quickstartUser' -p 'quickstartPwd1!' {app-group-command}
----
endif::[]

ifdef::addQuickstartAdmin[]
. Add the quickstart admin:
+
[source,subs="+quotes,attributes+",options="nowrap"]
----
$ {deploymentTargetDir}/server/bin/add-user.sh -a -u 'quickstartAdmin' -p 'adminPwd1!' {admin-group-command}
----
[NOTE]
====
For Windows, use the `__{jbossHomeName}__\bin\add-user.bat` script.
====
endif::[]

. Start the {productName} provisioned server, using the WildFly Maven Plugin `start` goal.
+
ifndef::deploymentDir[]
[source,subs="attributes+",options="nowrap"]
----
$ mvn wildfly:start {extraStartParams}
----
endif::[]
ifdef::deploymentDir[]
[source,subs="attributes+",options="nowrap"]
----
$ mvn -f {deploymentDir}/pom.xml wildfly:start {extraStartParams}
----
endif::[]

. Type the following command to run the integration tests.
+
[source,subs="attributes+",options="nowrap"]
----
$ mvn verify -Pintegration-testing {extraProvisioningTestParams}
----

. Shut down the {productName} provisioned server.
+
ifndef::deploymentDir[]
[source,subs="attributes+",options="nowrap"]
----
$ mvn wildfly:shutdown
----
endif::[]
ifdef::deploymentDir[]
[source,subs="attributes+",options="nowrap"]
----
$ mvn -f {deploymentDir}/pom.xml wildfly:shutdown
----
endif::[]

:leveloffset!:
// Bootable JAR
:leveloffset: +1

[[build_and_run_the_quickstart_with_bootable_jar]]
= Building and Running the quickstart application in a bootable JAR

//******************************************************************************
// This template sets attributes for the different standalone server profiles.
//
// You must define the `standalone-server-type`. Supported values are:
//    default
//    full
//    full-ha
//    ha
//    microprofile
//    custom
//******************************************************************************

// Standalone server with the default profile.
ifeval::["{standalone-server-type}"=="default"]
:serverProfile: default profile
:configFileName: standalone/configuration/standalone.xml
:serverArguments:
endif::[]

// Standalone server with the full profile.
ifeval::["{standalone-server-type}"=="full"]
:serverProfile: full profile
:configFileName: standalone/configuration/standalone-full.xml
:serverArguments:  -c standalone-full.xml
endif::[]

// Standalone server with the full HA profile.
ifeval::["{standalone-server-type}"=="full-ha"]
:serverProfile: full HA profile
:configFileName: standalone/configuration/standalone-full-ha.xml
:serverArguments:  -c standalone-full-ha.xml
endif::[]

// Start the standalone server with the HA profile.
ifeval::["{standalone-server-type}"=="ha"]
:serverProfile: HA profile
:configFileName: standalone/configuration/standalone-ha.xml
:serverArguments:  -c standalone-ha.xml
endif::[]

// Start the standalone server with the Eclipse MicroProfile profile.
ifeval::["{standalone-server-type}"=="microprofile"]
:serverProfile: MicroProfile profile
:configFileName: standalone/configuration/standalone-microprofile.xml
:serverArguments:  -c standalone-microprofile.xml
endif::[]

// Standalone server with the custom profile.
// NOTE: This profile requires that you define the `serverArguments` variable
// within the quickstart README.adoc file. For example:
//  :serverArguments: --server-config=../../docs/examples/configs/standalone-xts.xml
ifeval::["{standalone-server-type}"=="custom"]
:serverProfile: custom profile
endif::[]

// If there is no match, use the default profile.
ifndef::serverProfile[]
:standalone-server-type:  default
:serverProfile: default profile
:configFileName: standalone/configuration/standalone.xml
:serverArguments:
endif::serverProfile[]

ifndef::mavenServerProvisioningCommand[]
ifeval::["{archiveType}"=="ear"]
:mavenServerProvisioningCommand: clean install
endif::[]
ifeval::["{archiveType}"=="war"]
:mavenServerProvisioningCommand: clean package
endif::[]
ifeval::["{archiveType}"=="jar"]
:mavenServerProvisioningCommand: clean install
endif::[]
endif::mavenServerProvisioningCommand[]

You can use the WildFly Maven Plugin to build a {productName} bootable JAR to run this quickstart.

The quickstart `pom.xml` file contains a Maven profile named *bootable-jar*, which activates the bootable JAR packaging when provisioning {productName}, through the `<bootable-jar>true</bootable-jar>` configuration element:

ifndef::ProductRelease[]
[source,xml,subs="attributes+"]
----
<profile>
    <id>bootable-jar</id>
    <activation>
        <activeByDefault>true</activeByDefault>
    </activation>
    <build>
          <plugins>
              <plugin>
                  <groupId>org.wildfly.plugins</groupId>
                  <artifactId>wildfly-maven-plugin</artifactId>
                  <configuration>
                      <discover-provisioning-info>
                          <version>${version.server}</version>
                      </discover-provisioning-info>
                      <bootable-jar>true</bootable-jar>
                      <add-ons>...</add-ons>
                  </configuration>
                  <executions>
                      <execution>
                          <goals>
                              <goal>package</goal>
                          </goals>
                      </execution>
                  </executions>
              </plugin>
              ...
          </plugins>
    </build>
</profile>
----
endif::[]

ifdef::ProductRelease[]
[source,xml,subs="attributes+"]
----
<profile>
    <id>bootable-jar</id>
    <activation>
        <activeByDefault>true</activeByDefault>
    </activation>
    <build>
        <plugins>
            <plugin>
                <groupId>org.jboss.eap.plugins</groupId>
                <artifactId>eap-maven-plugin</artifactId>
                <configuration>
                    ...
                    <feature-packs>
                        <feature-pack>
                            <location>org.jboss.eap:wildfly-ee-galleon-pack</location>
                        </feature-pack>
                        ...
                    </feature-packs>
                    <layers>...</layers>
                    <bootable-jar>true</bootable-jar>
                </configuration>
                <executions>
                    <execution>
                        <goals>
                            <goal>package</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            ...
        </plugins>
    </build>
</profile>
----
endif::[]

[NOTE]
====
The *bootable-jar* profile is activate by default, and when built the {productName} bootable jar file is named `{artifactId}-bootable.jar`, and may be found in the `target` directory.
====

.Procedure

. Ensure the bootable jar is built.
+
[source,subs="attributes+",options="nowrap"]
----
$ mvn {mavenServerProvisioningCommand}
----

. Start the {productName} bootable jar use the WildFly Maven Plugin `start-jar` goal.
+
[source,subs="attributes+",options="nowrap"]
----
$ mvn wildfly:start-jar
----
+
[NOTE]
====
You may also start the bootable jar without Maven, using the `java` command.
[source,subs="attributes+",options="nowrap"]
----
$ java -jar target/{artifactId}-bootable.jar
----
====

. Run the integration tests use the `verify` goal, with the `integration-testing` profile activated.
+
[source,subs="attributes+",options="nowrap"]
----
$ mvn verify -Pintegration-testing
----

. Shut down the {productName} bootable jar use the WildFly Maven Plugin `shutdown` goal.
+
[source,options="nowrap"]
----
$ mvn wildfly:shutdown
----


:leveloffset!:
// OpenShift
:leveloffset: +1

:cloud-platform: OpenShift
:openshift: true
ifndef::helm-app-name[]
:helm-app-name: {artifactId}
endif::helm-app-name[]

[[build_and_run_the_quickstart_on_openshift]]
= Building and running the quickstart application with OpenShift
// The openshift profile
:leveloffset: +1

[[build-the-quickstart-for-openshift]]
== Build the {productName} Source-to-Image (S2I) Quickstart to {cloud-platform} with Helm Charts

On OpenShift, the S2I build with Apache Maven uses an `openshift` Maven profile to provision a {productName} server, deploy and run the quickstart in OpenShift environment.

ifndef::ProductRelease,EAPXPRelease[]
The server provisioning functionality is provided by the WildFly Maven Plugin, and you may find its configuration in the quickstart `pom.xml`:
endif::[]
ifdef::ProductRelease,EAPXPRelease[]
The server provisioning functionality is provided by the EAP Maven Plugin, and you may find its configuration in the quickstart `pom.xml`:
endif::[]

ifndef::ProductRelease,EAPXPRelease[]

[source,xml,subs="attributes+"]
----
<profile>
    <id>openshift</id>
    <build>
        <plugins>
            <plugin>
                <groupId>org.wildfly.plugins</groupId>
                <artifactId>wildfly-maven-plugin</artifactId>
                <configuration>
                    <discover-provisioning-info>
                        <version>${version.server}</version>
                        <context>cloud</context>
                    </discover-provisioning-info>
                    <add-ons>...</add-ons>
                </configuration>
                <executions>
                    <execution>
                        <goals>
                            <goal>package</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            ...
        </plugins>
    </build>
</profile>
----
You may note that unlike the `provisioned-server` profile it uses the cloud context which enables a configuration tuned for {cloud-platform} environment.
endif::ProductRelease,EAPXPRelease[]

ifdef::ProductRelease,EAPXPRelease[]
[source,xml,subs="attributes+"]
----
<profile>
    <id>openshift</id>
    <build>
        <plugins>
            <plugin>
                <groupId>org.jboss.eap.plugins</groupId>
                <artifactId>eap-maven-plugin</artifactId>
                <configuration>
                    ...
                    <feature-packs>
                        <feature-pack>
                            <location>org.jboss.eap:wildfly-ee-galleon-pack</location>
                        </feature-pack>
                        <feature-pack>
                            <location>org.jboss.eap.cloud:eap-cloud-galleon-pack</location>
                        </feature-pack>
                    </feature-packs>
                    <layers>...</layers>
                </configuration>
                <executions>
                    <execution>
                        <goals>
                            <goal>package</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            ...
        </plugins>
    </build>
</profile>
----
You may note that it uses the cloud feature pack which enables a configuration tuned for the {cloud-platform} environment.
endif::[]

:leveloffset: 1
// Getting Started with Helm
:leveloffset: +1

[[getting_started_with_helm]]
= Getting Started with {xpaasproduct-shortname} and Helm Charts

This section contains the basic instructions to build and deploy this quickstart to {xpaasproduct-shortname} using Helm Charts.

[[prerequisites_helm_openshift]]
== Prerequisites

ifndef::kubernetes[]
* You must be logged in OpenShift and have an `oc` client to connect to OpenShift
endif::[]
* https://helm.sh[Helm] must be installed to deploy the backend on {cloud-platform}.

Once you have installed Helm, you need to add the repository that provides Helm Charts for {ProductShortName}.

ifndef::ProductRelease[]
[source,options="nowrap"]
----
$ helm repo add wildfly https://docs.wildfly.org/wildfly-charts/
"wildfly" has been added to your repositories
$ helm search repo wildfly
NAME                    CHART VERSION   APP VERSION     DESCRIPTION
wildfly/wildfly         ...             ...            Build and Deploy WildFly applications on OpenShift
wildfly/wildfly-common  ...             ...            A library chart for WildFly-based applications
----
endif::[]
ifdef::ProductRelease[]
[source,options="nowrap",subs="+attributes"]
----
$ helm repo add jboss-eap https://jbossas.github.io/eap-charts/
"jboss-eap" has been added to your repositories
$ helm search repo jboss-eap
NAME                    CHART VERSION   APP VERSION     DESCRIPTION
{helmChartName}         ...             ...             A Helm chart to build and deploy EAP applications
----
endif::[]

:leveloffset: 1

ifdef::helm-install-prerequisites-openshift[]
// Additional steps needed before deploying in Helm
[[deploy_helm_prerequisites]]
:leveloffset: +1

:prereq-openshift: true
ifdef::prereq-openshift[]
:prereq-cloud-platform: OpenShift
:prereq-cloud-cli: oc
:prereq-suffix: openshift
endif::[]
ifdef::prereq-kubernetes[]
:prereq-cloud-platform: Kubernetes
:prereq-cloud-cli: kubectl
:prereq-suffix: kubernetes
endif::[]



=== Install OpenTelemetry Collector on OpenShift

The functionality of this quickstart depends on a running instance of the https://opentelemetry.io/docs/collector/[OpenTelemetry Collector].

To deploy and configure the OpenTelemetry Collector, you will need to apply a set of configurations to your {prereq-cloud-platform} cluster:

[source,options="nowrap",subs="+attributes"]
----
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: collector-config
data:
  collector.yml: |
    receivers:
      otlp:
        protocols:
          grpc:
              endpoint: "0.0.0.0:4317"
          http:
              endpoint: "0.0.0.0:4318"
    processors:
    exporters:
      debug:
        verbosity: detailed
      prometheus:
        endpoint: "0.0.0.0:1234"
    service:
      pipelines:
        metrics:
          receivers: [otlp]
          processors: []
          exporters: [debug,prometheus]
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetrycollector
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: opentelemetrycollector
  template:
    metadata:
      labels:
        app.kubernetes.io/name: opentelemetrycollector
    spec:
      containers:
        - name: otelcol
          args:
            - --config=/conf/collector.yml
          image: otel/opentelemetry-collector:0.115.1
          volumeMounts:
            - mountPath: /conf
              name: collector-config
      volumes:
        - configMap:
            items:
              - key: collector.yml
                path: collector.yml
            name: collector-config
          name: collector-config
---
apiVersion: v1
kind: Service
metadata:
  name: opentelemetrycollector
spec:
  ports:
    - name: otlp-grpc
      port: 4317
      protocol: TCP
      targetPort: 4317
    - name: otlp-http
      port: 4318
      protocol: TCP
      targetPort: 4318
    - name: prometheus
      port: 1234
      protocol: TCP
      targetPort: 1234
  selector:
    app.kubernetes.io/name: opentelemetrycollector
  type: ClusterIP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: otelcol-otlp-grpc
  labels:
    app.kubernetes.io/name: microprofile
spec:
  port:
    targetPort: otlp-grpc
  to:
    kind: Service
    name: opentelemetrycollector
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: otelcol-otlp-http
  labels:
    app.kubernetes.io/name: microprofile
spec:
  port:
    targetPort: otlp-http
  to:
    kind: Service
    name: opentelemetrycollector
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: otelcol-prometheus
  labels:
    app.kubernetes.io/name: microprofile
spec:
  port:
    targetPort: prometheus
  to:
    kind: Service
    name: opentelemetrycollector
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None
----

To make things simpler, you can find these commands in `charts/opentelemetry-collector-openshift.yaml`, and to apply them run the following command in your terminal:

[source,options="nowrap",subs="+attributes"]
----
$ {prereq-cloud-cli} apply -f charts/opentelemetry-collector-{prereq-suffix}.yaml
----

[NOTE]
====
When done with the quickstart, the `{prereq-cloud-cli} delete -f charts/opentelemetry-collector-{prereq-suffix}.yaml` command may be used to revert the applied changes.
====

:leveloffset: 1
endif::helm-install-prerequisites-openshift[]

//Prepare Helm for Quickstart Deployment
:leveloffset: +1

ifndef::helmSetWildFlyArgumentPrefix[]
// For use with nested Helm charts
:helmSetWildFlyArgumentPrefix:
endif::[]
ifeval::[{useHelmChartDir} == true]
:helm_chart_values: charts
endif::[]
ifndef::useHelmChartDir[]
:helm_chart_values: -f charts/helm.yaml {helmChartName}
endif::[]
ifdef::kubernetes[]
:helm-set-build-enabled: --set {helmSetWildFlyArgumentPrefix}build.enabled=false
:helm-set-deploy-route-enabled: --set {helmSetWildFlyArgumentPrefix}deploy.route.enabled=false
:helm-set-image-name: --set {helmSetWildFlyArgumentPrefix}image.name="localhost:5000/{artifactId}"
:helm-extra-arguments: {helm-set-build-enabled} {helm-set-deploy-route-enabled} {helm-set-image-name}
:cloud-cli: kubectl
endif::[]
ifndef::kubernetes[]
:helm-extra-arguments:
:cloud-cli: oc
endif::[]
[[deploy_helm]]
== Deploy the {ProductShortName} Source-to-Image (S2I) Quickstart to {cloud-platform} with Helm Charts

ifndef::kubernetes[]
Log in to your OpenShift instance using the `oc login` command.
endif::[]
The backend will be built and deployed on {cloud-platform} with a Helm Chart for {ProductShortName}.


ifndef::kubernetes[]
Navigate to the root directory of this quickstart and run the following command:
endif::[]
ifdef::kubernetes[]
Navigate to the root directory of this quickstart and run the following commands:

[source,options="nowrap",subs="+attributes"]
----
mvn -Popenshift package wildfly:image
----
This will use the `openshift` Maven profile we saw earlier to build the application, and create a Docker image containing the {productName} server with the application deployed. The name of the image will be `{artifactId}`.

Next we need to tag the image and make it available to Kubernetes. You can push it to a registry like `quay.io`. In this case we tag as `localhost:5000/{artifactId}:latest` and push it to the internal registry in our Kubernetes instance:

[source,options="nowrap",subs="+attributes"]
----
# Tag the image
docker tag {artifactId} localhost:5000/{artifactId}:latest
# Push the image to the registry
docker push localhost:5000/{artifactId}:latest
----

In the below call to `helm install` which deploys our application to Kubernetes, we are passing in some extra arguments to tweak the Helm build:

* `{helm-set-build-enabled}` - This turns off the s2i build for the Helm chart since Kubernetes, unlike OpenShift, does not have s2i. Instead, we are providing the image to use.
* `{helm-set-deploy-route-enabled}` - This disables route creation normally performed by the Helm chart. On Kubernetes we will use port-forwards instead to access our application, since routes are an OpenShift specific concept and thus not available on Kubernetes.
* `{helm-set-image-name}` - This tells the Helm chart to use the image we built, tagged and pushed to Kubernetes' internal registry above.

endif::[]
[source,options="nowrap",subs="+attributes"]
----
$ helm install {helm-app-name} {helm_chart_values} --wait --timeout=10m0s {helm-extra-arguments}
NAME: {helm-app-name}
...
STATUS: deployed
REVISION: 1
----

This command will return once the application has successfully deployed. In case of a timeout, you can check the status of the application with the following command in another terminal:

[source,options="nowrap",subs="+attributes"]
----
{cloud-cli} get deployment {helm-app-name}
----

The Helm Chart for this quickstart contains all the information to build an image from the source code using S2I on Java 17:


ifndef::requires-http-route[]
ifdef::useHelmChartDir[]
[source,yaml]
----
include::{docdir}/charts/Chart.yaml[]
----
endif::useHelmChartDir[]
ifndef::useHelmChartDir[]
[source,yaml]
----
build:
  uri: https://github.com/wildfly/quickstart.git
  ref: 36.x
  contextDir: micrometer
deploy:
  replicas: 1
  env:
    - name: OTEL_COLLECTOR_HOST
      value: "opentelemetrycollector"
----
endif::useHelmChartDir[]
endif::requires-http-route[]

ifdef::requires-http-route[]
[source,options="nowrap",subs="+attributes"]
----
build:
  uri: {githubRepoCodeUrl}
  ref: {WildFlyQuickStartRepoTag}
  contextDir: {artifactId}
deploy:
  replicas: 1
  route:
    tls:
      enabled: false
----
endif::requires-http-route[]

This will create a new deployment on {cloud-platform} and deploy the application.

If you want to see all the configuration elements to customize your deployment you can use the following command:
[source,options="nowrap",subs="+attributes"]
----
$ helm show readme {helmChartName}
----

ifdef::openshift[]
Get the URL of the route to the deployment.

[source,options="nowrap",subs="+attributes"]
----
$ oc get route {helm-app-name} -o jsonpath="{.spec.host}"
----
Access the application in your web browser using the displayed URL.
endif::[]
ifdef::kubernetes[]
To be able to connect to our application running in Kubernetes from outside, we need to set up a port-forward to the `{helm-app-name}` service created for us by the Helm chart.

This service will run on port `8080`, and we set up the port forward to also run on port `8080`:

[source,options="nowrap",subs="+attributes"]
----
kubectl port-forward service/{helm-app-name} 8080:8080
----
The server can now be accessed via `http://localhost:8080` from outside Kubernetes. Note that the command to create the port-forward will not return, so it is easiest to run this in a separate terminal.

endif::[]

ifdef::openshift+post-helm-install-actions-openshift[]
include::{post-helm-install-actions-openshift}[leveloffset=+1]
endif::openshift+post-helm-install-actions-openshift[]
ifdef::kubernetes+post-helm-install-actions-kubernetes[]
include::{post-helm-install-actions-kubernetes}[leveloffset=+1]
endif::kubernetes+post-helm-install-actions-kubernetes[]

:leveloffset: 1

// Testing on Openshift
:leveloffset: +1

[[run_the_integration_tests_with_openshift]]
= Run the Integration Tests with OpenShift
The integration tests included with this quickstart, which verify that the quickstart runs correctly, may also be run with the quickstart running on OpenShift.
[NOTE]
====
The integration tests expect a deployed application, so make sure you have deployed the quickstart on OpenShift before you begin.
====

ifdef::extra-testing-actions-openshift[]
include::{extra-testing-actions-openshift}[leveloffset=+1]
endif::extra-testing-actions-openshift[]

ifndef::extra-test-arguments-openshift[:extra-test-arguments-openshift:]

Run the integration tests using the following command to run the `verify` goal with the `integration-testing` profile activated and the proper URL:
ifndef::requires-http-route[]
[source,options="nowrap",subs="+attributes"]
----
$ mvn verify -Pintegration-testing -Dserver.host=https://$(oc get route {helm-app-name} --template='{{ .spec.host }}') {extra-test-arguments-openshift}
----
endif::requires-http-route[]
ifdef::requires-http-route[]
[source,options="nowrap",subs="+attributes"]
----
$ mvn verify -Pintegration-testing -Dserver.host=http://$(oc get route {helm-app-name} --template='{{ .spec.host }}') {extra-test-arguments-openshift}
----
endif::requires-http-route[]

[NOTE]
====
The tests are using SSL to connect to the quickstart running on OpenShift. So you need the certificates to be trusted by the machine the tests are run from.
====

:leveloffset: 1

//Prepare Helm for Quickstart Deployment
:leveloffset: +1

[[undeploy_helm]]
== Undeploy the {ProductShortName} Source-to-Image (S2I) Quickstart from {cloud-platform} with Helm Charts

[source,options="nowrap",subs="+attributes"]
----
$ helm uninstall {helm-app-name}
----
ifdef::kubernetes[]
To stop the port forward you created earlier use:
[source,options="nowrap",subs="+attributes"]
----
$ kubectl port-forward service/{helm-app-name} 8080:8080
----
endif::[]

:leveloffset: 1

// Unset the attribute
:!openshift:

:leveloffset!:
ifndef::ProductRelease,EAPXPRelease[]
//Kubernetes
:leveloffset: +1

:cloud-platform: Kubernetes
:kubernetes: true
ifndef::helm-app-name[]
:helm-app-name: {artifactId}
endif::helm-app-name[]

[[build_and_run_the_quickstart_on_kubernetes]]
= Building and running the quickstart application with Kubernetes
// The openshift profile
:leveloffset: +1

[[build-the-quickstart-for-kubernetes]]
== Build the {productName} Quickstart to Kubernetes with Helm Charts

For Kubernetes, the build with Apache Maven uses an `openshift` Maven profile to provision a {productName} server, suitable for running on Kubernetes.

ifndef::ProductRelease,EAPXPRelease[]
The server provisioning functionality is provided by the WildFly Maven Plugin, and you may find its configuration in the quickstart `pom.xml`:
endif::[]
ifdef::ProductRelease,EAPXPRelease[]
The server provisioning functionality is provided by the EAP Maven Plugin, and you may find its configuration in the quickstart `pom.xml`:
endif::[]

ifndef::ProductRelease,EAPXPRelease[]

[source,xml,subs="attributes+"]
----
<profile>
    <id>openshift</id>
    <build>
        <plugins>
            <plugin>
                <groupId>org.wildfly.plugins</groupId>
                <artifactId>wildfly-maven-plugin</artifactId>
                <configuration>
                    <discover-provisioning-info>
                        <version>${version.server}</version>
                        <context>cloud</context>
                    </discover-provisioning-info>
                    <add-ons>...</add-ons>
                </configuration>
                <executions>
                    <execution>
                        <goals>
                            <goal>package</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            ...
        </plugins>
    </build>
</profile>
----
You may note that unlike the `provisioned-server` profile it uses the cloud context which enables a configuration tuned for {cloud-platform} environment.
endif::ProductRelease,EAPXPRelease[]

ifdef::ProductRelease,EAPXPRelease[]
[source,xml,subs="attributes+"]
----
<profile>
    <id>openshift</id>
    <build>
        <plugins>
            <plugin>
                <groupId>org.jboss.eap.plugins</groupId>
                <artifactId>eap-maven-plugin</artifactId>
                <configuration>
                    ...
                    <feature-packs>
                        <feature-pack>
                            <location>org.jboss.eap:wildfly-ee-galleon-pack</location>
                        </feature-pack>
                        <feature-pack>
                            <location>org.jboss.eap.cloud:eap-cloud-galleon-pack</location>
                        </feature-pack>
                    </feature-packs>
                    <layers>...</layers>
                </configuration>
                <executions>
                    <execution>
                        <goals>
                            <goal>package</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            ...
        </plugins>
    </build>
</profile>
----
You may note that it uses the cloud feature pack which enables a configuration tuned for the {cloud-platform} environment.
endif::[]

:leveloffset: 1
// Getting Started with Helm
:leveloffset: +1

[[getting_started_with_helm]]
= Getting Started with Kubernetes and Helm Charts

This section contains the basic instructions to build and deploy this quickstart to Kubernetes using Helm Charts.

== Install Kubernetes
In this example we are using https://github.com/kubernetes/minikube[Minikube] as our Kubernetes provider. See the https://minikube.sigs.k8s.io/docs/start/[Minikube Getting Started guide] for how to install it. After installing it, we start it with 4GB of memory.

[source,options="nowrap",subs="+attributes"]
----
minikube start --memory='4gb'
----
The above command should work if you have Docker installed on your machine. If, you are using https://podman-desktop.io[Podman] instead of Docker, you will also need to pass in `--driver=podman`, as covered in the https://minikube.sigs.k8s.io/docs/handbook/config/[Minikube documentation].

Once Minikube has started, we need to enable its https://minikube.sigs.k8s.io/docs/handbook/registry/[registry] since that is where we will push the image needed to deploy the quickstart, and where we will tell the Helm charts to download it from.

[source,options="nowrap",subs="+attributes"]
----
minikube addons enable registry
----

In order to be able to push images to the registry we need to make it accessible from outside Kubernetes. How we do this depends on your operating system. All the below examples will expose it at `localhost:5000`

[source,options="nowrap",subs="+attributes"]
----
# On Mac:
docker run --rm -it --network=host alpine ash -c "apk add socat && socat TCP-LISTEN:5000,reuseaddr,fork TCP:$(minikube ip):5000"

# On Linux:
kubectl port-forward --namespace kube-system service/registry 5000:80 &

# On Windows:
kubectl port-forward --namespace kube-system service/registry 5000:80
docker run --rm -it --network=host alpine ash -c "apk add socat && socat TCP-LISTEN:5000,reuseaddr,fork TCP:host.docker.internal:5000"
----

[[prerequisites_helm_openshift]]
== Prerequisites

ifndef::kubernetes[]
* You must be logged in OpenShift and have an `oc` client to connect to OpenShift
endif::[]
* https://helm.sh[Helm] must be installed to deploy the backend on {cloud-platform}.

Once you have installed Helm, you need to add the repository that provides Helm Charts for {ProductShortName}.

ifndef::ProductRelease[]
[source,options="nowrap"]
----
$ helm repo add wildfly https://docs.wildfly.org/wildfly-charts/
"wildfly" has been added to your repositories
$ helm search repo wildfly
NAME                    CHART VERSION   APP VERSION     DESCRIPTION
wildfly/wildfly         ...             ...            Build and Deploy WildFly applications on OpenShift
wildfly/wildfly-common  ...             ...            A library chart for WildFly-based applications
----
endif::[]
ifdef::ProductRelease[]
[source,options="nowrap",subs="+attributes"]
----
$ helm repo add jboss-eap https://jbossas.github.io/eap-charts/
"jboss-eap" has been added to your repositories
$ helm search repo jboss-eap
NAME                    CHART VERSION   APP VERSION     DESCRIPTION
{helmChartName}         ...             ...             A Helm chart to build and deploy EAP applications
----
endif::[]

:leveloffset: 1

ifdef::helm-install-prerequisites-kubernetes[]
// Additional steps needed before deploying in Helm
[[deploy_helm_prerequisites]]
:leveloffset: +1

:prereq-kubernetes: true
ifdef::prereq-openshift[]
:prereq-cloud-platform: OpenShift
:prereq-cloud-cli: oc
:prereq-suffix: openshift
endif::[]
ifdef::prereq-kubernetes[]
:prereq-cloud-platform: Kubernetes
:prereq-cloud-cli: kubectl
:prereq-suffix: kubernetes
endif::[]



=== Install OpenTelemetry Collector on OpenShift

The functionality of this quickstart depends on a running instance of the https://opentelemetry.io/docs/collector/[OpenTelemetry Collector].

To deploy and configure the OpenTelemetry Collector, you will need to apply a set of configurations to your {prereq-cloud-platform} cluster:

[source,options="nowrap",subs="+attributes"]
----
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: collector-config
data:
  collector.yml: |
    receivers:
      otlp:
        protocols:
          grpc:
              endpoint: "0.0.0.0:4317"
          http:
              endpoint: "0.0.0.0:4318"
    processors:
    exporters:
      debug:
        verbosity: detailed
      prometheus:
        endpoint: "0.0.0.0:1234"
    service:
      pipelines:
        metrics:
          receivers: [otlp]
          processors: []
          exporters: [debug,prometheus]
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetrycollector
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: opentelemetrycollector
  template:
    metadata:
      labels:
        app.kubernetes.io/name: opentelemetrycollector
    spec:
      containers:
        - name: otelcol
          args:
            - --config=/conf/collector.yml
          image: otel/opentelemetry-collector:0.115.1
          volumeMounts:
            - mountPath: /conf
              name: collector-config
      volumes:
        - configMap:
            items:
              - key: collector.yml
                path: collector.yml
            name: collector-config
          name: collector-config
---
apiVersion: v1
kind: Service
metadata:
  name: opentelemetrycollector
spec:
  ports:
    - name: otlp-grpc
      port: 4317
      protocol: TCP
      targetPort: 4317
    - name: otlp-http
      port: 4318
      protocol: TCP
      targetPort: 4318
    - name: prometheus
      port: 1234
      protocol: TCP
      targetPort: 1234
  selector:
    app.kubernetes.io/name: opentelemetrycollector
  type: ClusterIP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: otelcol-otlp-grpc
  labels:
    app.kubernetes.io/name: microprofile
spec:
  port:
    targetPort: otlp-grpc
  to:
    kind: Service
    name: opentelemetrycollector
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: otelcol-otlp-http
  labels:
    app.kubernetes.io/name: microprofile
spec:
  port:
    targetPort: otlp-http
  to:
    kind: Service
    name: opentelemetrycollector
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: otelcol-prometheus
  labels:
    app.kubernetes.io/name: microprofile
spec:
  port:
    targetPort: prometheus
  to:
    kind: Service
    name: opentelemetrycollector
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None
----

To make things simpler, you can find these commands in `charts/opentelemetry-collector-openshift.yaml`, and to apply them run the following command in your terminal:

[source,options="nowrap",subs="+attributes"]
----
$ {prereq-cloud-cli} apply -f charts/opentelemetry-collector-{prereq-suffix}.yaml
----

[NOTE]
====
When done with the quickstart, the `{prereq-cloud-cli} delete -f charts/opentelemetry-collector-{prereq-suffix}.yaml` command may be used to revert the applied changes.
====

:leveloffset: 1
endif::helm-install-prerequisites-kubernetes[]

//Prepare Helm for Quickstart Deployment
:leveloffset: +1

ifndef::helmSetWildFlyArgumentPrefix[]
// For use with nested Helm charts
:helmSetWildFlyArgumentPrefix:
endif::[]
ifeval::[{useHelmChartDir} == true]
:helm_chart_values: charts
endif::[]
ifndef::useHelmChartDir[]
:helm_chart_values: -f charts/helm.yaml {helmChartName}
endif::[]
ifdef::kubernetes[]
:helm-set-build-enabled: --set {helmSetWildFlyArgumentPrefix}build.enabled=false
:helm-set-deploy-route-enabled: --set {helmSetWildFlyArgumentPrefix}deploy.route.enabled=false
:helm-set-image-name: --set {helmSetWildFlyArgumentPrefix}image.name="localhost:5000/{artifactId}"
:helm-extra-arguments: {helm-set-build-enabled} {helm-set-deploy-route-enabled} {helm-set-image-name}
:cloud-cli: kubectl
endif::[]
ifndef::kubernetes[]
:helm-extra-arguments:
:cloud-cli: oc
endif::[]
[[deploy_helm]]
== Deploy the {ProductShortName} Source-to-Image (S2I) Quickstart to {cloud-platform} with Helm Charts

ifndef::kubernetes[]
Log in to your OpenShift instance using the `oc login` command.
endif::[]
The backend will be built and deployed on {cloud-platform} with a Helm Chart for {ProductShortName}.


ifndef::kubernetes[]
Navigate to the root directory of this quickstart and run the following command:
endif::[]
ifdef::kubernetes[]
Navigate to the root directory of this quickstart and run the following commands:

[source,options="nowrap",subs="+attributes"]
----
mvn -Popenshift package wildfly:image
----
This will use the `openshift` Maven profile we saw earlier to build the application, and create a Docker image containing the {productName} server with the application deployed. The name of the image will be `{artifactId}`.

Next we need to tag the image and make it available to Kubernetes. You can push it to a registry like `quay.io`. In this case we tag as `localhost:5000/{artifactId}:latest` and push it to the internal registry in our Kubernetes instance:

[source,options="nowrap",subs="+attributes"]
----
# Tag the image
docker tag {artifactId} localhost:5000/{artifactId}:latest
# Push the image to the registry
docker push localhost:5000/{artifactId}:latest
----

In the below call to `helm install` which deploys our application to Kubernetes, we are passing in some extra arguments to tweak the Helm build:

* `{helm-set-build-enabled}` - This turns off the s2i build for the Helm chart since Kubernetes, unlike OpenShift, does not have s2i. Instead, we are providing the image to use.
* `{helm-set-deploy-route-enabled}` - This disables route creation normally performed by the Helm chart. On Kubernetes we will use port-forwards instead to access our application, since routes are an OpenShift specific concept and thus not available on Kubernetes.
* `{helm-set-image-name}` - This tells the Helm chart to use the image we built, tagged and pushed to Kubernetes' internal registry above.

endif::[]
[source,options="nowrap",subs="+attributes"]
----
$ helm install {helm-app-name} {helm_chart_values} --wait --timeout=10m0s {helm-extra-arguments}
NAME: {helm-app-name}
...
STATUS: deployed
REVISION: 1
----

This command will return once the application has successfully deployed. In case of a timeout, you can check the status of the application with the following command in another terminal:

[source,options="nowrap",subs="+attributes"]
----
{cloud-cli} get deployment {helm-app-name}
----

The Helm Chart for this quickstart contains all the information to build an image from the source code using S2I on Java 17:


ifndef::requires-http-route[]
ifdef::useHelmChartDir[]
[source,yaml]
----
include::{docdir}/charts/Chart.yaml[]
----
endif::useHelmChartDir[]
ifndef::useHelmChartDir[]
[source,yaml]
----
build:
  uri: https://github.com/wildfly/quickstart.git
  ref: 36.x
  contextDir: micrometer
deploy:
  replicas: 1
  env:
    - name: OTEL_COLLECTOR_HOST
      value: "opentelemetrycollector"
----
endif::useHelmChartDir[]
endif::requires-http-route[]

ifdef::requires-http-route[]
[source,options="nowrap",subs="+attributes"]
----
build:
  uri: {githubRepoCodeUrl}
  ref: {WildFlyQuickStartRepoTag}
  contextDir: {artifactId}
deploy:
  replicas: 1
  route:
    tls:
      enabled: false
----
endif::requires-http-route[]

This will create a new deployment on {cloud-platform} and deploy the application.

If you want to see all the configuration elements to customize your deployment you can use the following command:
[source,options="nowrap",subs="+attributes"]
----
$ helm show readme {helmChartName}
----

ifdef::openshift[]
Get the URL of the route to the deployment.

[source,options="nowrap",subs="+attributes"]
----
$ oc get route {helm-app-name} -o jsonpath="{.spec.host}"
----
Access the application in your web browser using the displayed URL.
endif::[]
ifdef::kubernetes[]
To be able to connect to our application running in Kubernetes from outside, we need to set up a port-forward to the `{helm-app-name}` service created for us by the Helm chart.

This service will run on port `8080`, and we set up the port forward to also run on port `8080`:

[source,options="nowrap",subs="+attributes"]
----
kubectl port-forward service/{helm-app-name} 8080:8080
----
The server can now be accessed via `http://localhost:8080` from outside Kubernetes. Note that the command to create the port-forward will not return, so it is easiest to run this in a separate terminal.

endif::[]

ifdef::openshift+post-helm-install-actions-openshift[]
include::{post-helm-install-actions-openshift}[leveloffset=+1]
endif::openshift+post-helm-install-actions-openshift[]
ifdef::kubernetes+post-helm-install-actions-kubernetes[]
include::{post-helm-install-actions-kubernetes}[leveloffset=+1]
endif::kubernetes+post-helm-install-actions-kubernetes[]

:leveloffset: 1

// Testing on Openshift
:leveloffset: +1

[[run_the_integration_tests_with_kubernetes]]
= Run the Integration Tests with Kubernetes
The integration tests included with this quickstart, which verify that the quickstart runs correctly, may also be run with the quickstart running on Kubernetes.
[NOTE]
====
The integration tests expect a deployed application, so make sure you have deployed the quickstart on Kubernetes before you begin.
====

ifdef::extra-testing-actions-kubernetes[]
include::{extra-testing-actions-kubernetes}[leveloffset=+1]
endif::extra-testing-actions-kubernetes[]

ifndef::extra-test-arguments-kubernetes[:extra-test-arguments-kubernetes:]

Run the integration tests using the following command to run the `verify` goal with the `integration-testing` profile activated and the proper URL:
[source,options="nowrap",subs="+attributes"]
----
$ mvn verify -Pintegration-testing -Dserver.host=http://localhost:8080 {extra-test-arguments-kubernetes}
----


:leveloffset: 1

//Prepare Helm for Quickstart Deployment
:leveloffset: +1

[[undeploy_helm]]
== Undeploy the {ProductShortName} Source-to-Image (S2I) Quickstart from {cloud-platform} with Helm Charts

[source,options="nowrap",subs="+attributes"]
----
$ helm uninstall {helm-app-name}
----
ifdef::kubernetes[]
To stop the port forward you created earlier use:
[source,options="nowrap",subs="+attributes"]
----
$ kubectl port-forward service/{helm-app-name} 8080:8080
----
endif::[]

:leveloffset: 1

// Unset the attribute
:!kubernetes:

:leveloffset!:
endif::[]
== Conclusion

Micrometer provides a de facto standard way of capturing and publishing metrics to the monitoring solution of your choice. {productName} provides a convenient, out-of-the-box integration of Micrometer to make it easier to capture those metrics and monitor your application's health and performance. For more information on Micrometer, please refer to the project's https://micrometer.io[website].
